{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2 id=\"load_dataset\">Load the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Here we will load from github or do API or something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load Data From CSV File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    sec      open      high       low     close  volume\n",
      "0  2023.01.02  18:00  1826.837  1827.337  1826.617  1826.637       0\n",
      "1  2023.01.02  18:01  1826.537  1827.357  1826.137  1826.537       0\n",
      "2  2023.01.02  18:02  1826.137  1826.737  1826.137  1826.737       0\n",
      "3  2023.01.02  18:05  1827.187  1828.867  1827.187  1828.738       0\n",
      "4  2023.01.02  18:06  1828.758  1829.958  1828.758  1829.497       0\n",
      "date      0\n",
      "sec       0\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "date       object\n",
      "sec        object\n",
      "open      float64\n",
      "high      float64\n",
      "low       float64\n",
      "close     float64\n",
      "volume      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "xau23 = pd.read_csv(\"./data/xau23.csv\", header=None)\n",
    "xau22 = pd.read_csv(\"./data/xau22.csv\", header=None)\n",
    "xau21 = pd.read_csv(\"./data/xau21.csv\", header=None)\n",
    "\n",
    "# create a columns name\n",
    "cname= [\"date\",\"sec\",\"open\", \"high\", \"low\", \"close\",\"volume\"]\n",
    "xau23.columns = cname\n",
    "xau22.columns = cname\n",
    "xau21.columns = cname\n",
    "\n",
    "# union the data set \n",
    "xau_data = pd.concat([xau23,xau22,xau21])\n",
    "print(xau_data.head())\n",
    "\n",
    "# Check for N/A\n",
    "print(xau_data.isna().sum())\n",
    "# Check for types\n",
    "print(xau_data.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime    datetime64[ns]\n",
      "open               float64\n",
      "high               float64\n",
      "low                float64\n",
      "close              float64\n",
      "volume               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# combine the date and time columns into a single string\n",
    "xau_data['datetime'] = xau_data['date'] + xau_data['sec']\n",
    "\n",
    "# convert the combined string to a Pandas datetime object\n",
    "xau_data['datetime'] = pd.to_datetime(xau_data['datetime'], format = '%Y.%m.%d%H:%M')\n",
    "\n",
    "# reselect the data\n",
    "xau_data = xau_data[['datetime','open','high','low','close','volume']]\n",
    "\n",
    "# sort the data ascending\n",
    "xau_data = xau_data.sort_values(by='datetime',ascending=True)\n",
    "\n",
    "print(xau_data.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling into 1 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open      6170\n",
      "high      6170\n",
      "low       6170\n",
      "close     6170\n",
      "volume       0\n",
      "dtype: int64\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "12700\n",
      "                         open      high       low     close  volume\n",
      "datetime                                                           \n",
      "2021-01-03 18:00:00  1904.998  1914.858  1903.288  1913.035       0\n",
      "2021-01-03 19:00:00  1913.025  1913.508  1909.858  1913.278       0\n",
      "2021-01-03 20:00:00  1913.278  1918.655  1912.124  1916.524       0\n",
      "2021-01-03 21:00:00  1916.558  1921.775  1915.394  1921.675       0\n",
      "2021-01-03 22:00:00  1921.695  1925.145  1920.675  1923.120       0\n"
     ]
    }
   ],
   "source": [
    "# set the datetime column as the index\n",
    "xau_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# resample data into 1hour interval\n",
    "xau_data = xau_data.resample('1H').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last','volume':'sum'})\n",
    "\n",
    "print(xau_data.isnull().sum())\n",
    "\n",
    "# drop null value (Market Close on those days)\n",
    "xau_data = xau_data.dropna()\n",
    "print(xau_data.isnull().sum())\n",
    "\n",
    "print(len(xau_data))\n",
    "print(xau_data.head())\n",
    "\n",
    "\n",
    "#### Adding shift windows\n",
    "# targets, check the highest gold price attained in the next 4 hours\n",
    "df = xau_data\n",
    "highs = df['high'].rolling(window=4).max().shift(-4)\n",
    "lows = df['low'].rolling(window=4).min().shift(-4)\n",
    "\n",
    "# create new columns for conditions\n",
    "df['high_close_diff'] = highs - df['close'].shift(1)\n",
    "df['low_close_diff'] = lows - df['close'].shift(1)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Condition for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3914\n",
      "0 8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkong\\AppData\\Local\\Temp\\ipykernel_11476\\2025488886.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df.apply(reco,axis=1)\n"
     ]
    }
   ],
   "source": [
    "def reco(row):\n",
    "    if row.low_close_diff <= -3.3:\n",
    "        return 0\n",
    "    elif row.high_close_diff >= 4.3: #I use 3 plus .3 for bit offer off-set, and 1 in case of delay ordering cause price to change.\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['target'] = df.apply(reco,axis=1)\n",
    "\n",
    "\n",
    "print('1', (df['target'] == 1).sum())\n",
    "print('0', (df['target'] == 0).sum())\n",
    "\n",
    "df_simu = df\n",
    "df = df[['open','high','low','close','target']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Training, Cross Validation, and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12695\n",
      "number of training set:  8252\n",
      "number of cross validation set:  2539\n",
      "number of test set:  1903\n",
      "sum 12694\n",
      "Training set start: 2021-01-03 19:00:00 \n",
      " Training set end: 2022-05-26 18:00:00\n",
      "cross validation set start: 2022-05-26 19:00:00 \n",
      " cross validation set end: 2022-10-28 11:00:00\n",
      "test set start: 2022-10-28 12:00:00 \n",
      " test set end: 2023-02-28 19:00:00\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(n)\n",
    "\n",
    "train_n = int(round(n*0.65,0))\n",
    "print('number of training set: ', train_n)\n",
    "\n",
    "cross_n = int(round(n*0.20,0))\n",
    "print('number of cross validation set: ', cross_n)\n",
    "\n",
    "test_n = int(round(n*0.15-1,0))\n",
    "print('number of test set: ', test_n)\n",
    "\n",
    "print('sum', train_n + cross_n + test_n)\n",
    "\n",
    "\n",
    "train_start = str(df.iloc[0].name)\n",
    "train_end = str(df.iloc[train_n].name)\n",
    "print('Training set start: {0} \\n Training set end: {1}'.format(train_start,train_end))\n",
    "#print(df.loc[train_start:train_end])\n",
    "\n",
    "cross_start = str(df.iloc[train_n+1].name)\n",
    "cross_end = str(df.iloc[train_n+cross_n].name)\n",
    "print('cross validation set start: {0} \\n cross validation set end: {1}'.format(cross_start,cross_end))\n",
    "#print(df.loc[cross_start:cross_end])\n",
    "\n",
    "test_start = str(df.iloc[train_n + cross_n +1].name)\n",
    "test_end = str(df.iloc[n-1].name)\n",
    "print('test set start: {0} \\n test set end: {1}'.format(test_start,test_end))\n",
    "#print(df.loc[test_start:])\n",
    "\n",
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train = np.asarray(X.loc[train_start:train_end])\n",
    "y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "X_test = np.asarray(X.loc[test_start:])\n",
    "y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "\n",
    "#Extra for GridSearch\n",
    "X_train_cv = np.asarray(X.loc[train_start:cross_end])\n",
    "y_train_cv = np.asarray(y.loc[train_start:cross_end])\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape, X_cross.shape, y_cross.shape, X_test.shape, y_test.shape, X_train_cv.shape, y_train_cv.shape\n",
    "\n",
    "\n",
    "# Below is for later simulation\n",
    "df_simu_test = df_simu.loc[test_start:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####เอาไว้เรียนรู้ทำความเข้าใจ ตรงนี้สำคัญ ยากและลืมง่าย\n",
    "\n",
    "#Create a sample dataframe\n",
    "df = pd.DataFrame({'value': [2, 4, 5, 7, 6, 8, 9, 10, 11, 12, 9, 8, 6, 4, 3]})\n",
    "\n",
    "#Apply rolling window with current row included and apply max function\n",
    "window_period = 4\n",
    "df['rolling_min'] = df['value'].rolling(window=4).min().shift(-3)\n",
    "df['rolling_max'] = df['value'].rolling(window=4).max().shift(-3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), \"C\" is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "The hyperparameter \"C\" in SVM controls the misclassification penalty. A smaller value of \"C\" allows more misclassifications on the training data, while a larger value of \"C\" penalizes misclassifications more heavily. In other words, a smaller value of \"C\" creates a wider margin, allowing more data points to fall within the margin, but may result in lower accuracy on the training data. Conversely, a larger value of \"C\" creates a narrower margin, reducing the number of misclassifications but may lead to overfitting.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Large C >>> Larger penalizes >>> complex model >>> lead to overfittting <br>\n",
    "Small C >>> Smaller penalizes >>> simple model >>> underfitting\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01, Train score=0.6967163455713073, cross score=0.7113036628593935\n",
      "C=0.1, Train score=0.7380346540651884, cross score=0.7526585269791256\n",
      "C=0.5, Train score=0.7568157033805889, cross score=0.7593540764080347\n",
      "C=1, Train score=0.760208409063371, cross score=0.7636864907443875\n",
      "C=1.5, Train score=0.7596025687628741, cross score=0.7628987790468689\n",
      "C=2, Train score=0.7603295771234703, cross score=0.7625049231981095\n",
      "C=10, Train score=0.7629952744456562, cross score=0.7625049231981095\n",
      "The best 'C' is: 10\n",
      "score: 0.76\n",
      "Accuracy_score:  0.76\n",
      "weight avg 0.74\n",
      "jaccard:  0.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84      1794\n",
      "           1       0.66      0.39      0.49       745\n",
      "\n",
      "    accuracy                           0.76      2539\n",
      "   macro avg       0.72      0.65      0.67      2539\n",
      "weighted avg       0.75      0.76      0.74      2539\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of hyperparameters to loop through\n",
    "#################larger value of C imposes less regularization on the model\n",
    "Cs = [0.01, 0.1,0.5, 1, 1.5, 2, 10]\n",
    "\n",
    "# Initialize variables to keep track of the best hyperparameters and their corresponding score\n",
    "best_score = 0\n",
    "best_C = None\n",
    "max_iter = 20000\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "# Loop through the hyperparameters\n",
    "for C in Cs:\n",
    "    lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter)\n",
    "    lsvm.fit(X_train_n, y_train)\n",
    "    train_score = lsvm.score(features_X_train, target_y_train)\n",
    "    cross_score = lsvm.score(features_X_cross, target_y_cross)\n",
    "    print(f\"C={C}, Train score={train_score}, cross score={cross_score}\")\n",
    "    \n",
    "    # Update the best hyperparameters and their corresponding score if applicable\n",
    "    if train_score > best_score:\n",
    "        best_score = train_score\n",
    "        best_C = C\n",
    "\n",
    "print(\"The best 'C' is:\", best_C)\n",
    "# Create the LinearSVC model with normalization and the best value of C\n",
    "lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=best_C, max_iter=max_iter)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lsvm.fit(features_X_train, target_y_train)\n",
    "\n",
    "\n",
    "#EVALUATION\n",
    "test_score = round(lsvm.score(features_X_cross, target_y_cross),2)\n",
    "print(\"score:\", test_score)\n",
    "\n",
    "\n",
    "yhat_m1 = lsvm.predict(features_X_cross)\n",
    "accuracy_m1 = round(accuracy_score(target_y_cross,yhat_m1),2)\n",
    "print('Accuracy_score: ', accuracy_m1)\n",
    "\n",
    "f1 = round(f1_score(target_y_cross, yhat_m1, average='weighted'), 2)\n",
    "print('weight avg', f1)\n",
    "\n",
    "jaccard = round(jaccard_score(target_y_cross, yhat_m1,pos_label=1),2)\n",
    "print('jaccard: ', jaccard)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross, yhat_m1))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(lsvm, features_X_cross, target_y_cross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----cross-----\n",
      "The value should be:>> yhat = 0: 2093<<, >> yhat = 1: 446<<\n",
      "2093\n",
      "446\n",
      "-----test-----\n",
      "1578\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "X_test_simu = X.loc[test_start:]\n",
    "X_test_simu_n = normalize_data_new(X_test_simu,X_train_means,X_train_stds)\n",
    "\n",
    "X_cross_simu = X.loc[cross_start:cross_end]\n",
    "X_cross_simu_n = normalize_data_new(X_cross_simu,X_train_means,X_train_stds)\n",
    "\n",
    "def predict_count(data_simu):\n",
    "    adding = []\n",
    "    for i in range(0,len(data_simu)):\n",
    "        prediction = str(lsvm.predict(data_simu.iloc[i].values.reshape(1,-1))).strip('[]')\n",
    "        adding.append(prediction)\n",
    "    print(adding.count('0'))\n",
    "    print(adding.count('1'))\n",
    "\n",
    "\n",
    "print('-----cross-----')\n",
    "print('The value should be:>> yhat = 0: {0}<<, >> yhat = 1: {1}<<'.format(np.count_nonzero(yhat_m1 == 0),np.count_nonzero(yhat_m1 == 1)))\n",
    "predict_count(X_cross_simu_n)\n",
    "\n",
    "print('-----test-----')\n",
    "predict_count(X_test_simu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simu(model, data_simu_x_n, data_simu_y, initial_capital):\n",
    "    capital = initial_capital\n",
    "    win_count = 0\n",
    "    lost_count = 0\n",
    "    no_order_count = 0\n",
    "    record_result = []\n",
    "    record_capital = []\n",
    "    record_date = []\n",
    "    consecutive_lost = 0\n",
    "    consecutive_lost_max = 0\n",
    "    for i in range(0, len(data_simu_x_n)):\n",
    "        prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        \n",
    "        if prediction == 1 and data_simu_y[i] == 1:\n",
    "            outcome = 3\n",
    "            capital += outcome\n",
    "            win_count += 1\n",
    "            order_record = 'win-------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'win'\n",
    "        elif prediction == 1 and data_simu_y[i] == 0:\n",
    "            outcome = -3.3\n",
    "            capital += outcome\n",
    "            lost_count += 1\n",
    "            order_record = 'lost------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'lost'\n",
    "        elif prediction == 0:\n",
    "            no_order_count +=1\n",
    "            order_record = 'no order--- prediction = {0}, actual = {1}'.format(prediction, data_simu_y[i])\n",
    "            capital = capital\n",
    "            result = 'no order'\n",
    "        else:\n",
    "            raise ValueError('no condition met')\n",
    "        record_date.append(data_simu_x_n.iloc[i].name)\n",
    "        record_result.append(order_record)\n",
    "        record_capital.append(capital)\n",
    "\n",
    "        #Calculate Consecutive Lost\n",
    "        if result == 'lost':\n",
    "            consecutive_lost += -3.3\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "        if result == 'win':\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "            consecutive_lost = 0\n",
    "\n",
    "    total_return = ((capital - initial_capital) / initial_capital)*100\n",
    "    sim_df = pd.DataFrame({'record_date':record_date,\n",
    "                           'record_result':record_result,\n",
    "                           'record_capital': record_capital})\n",
    "    sim_df.set_index('record_date', inplace=True)\n",
    "\n",
    "    return win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Return: 84.0%\n",
      "Final Capital: $921.0\n",
      "Win Count:  237\n",
      "Lost Count:  88\n",
      "No Order Count:  1578\n",
      "Max Consecutive Lost: 5.0 trades\n",
      "Total day on trading: 123 days\n"
     ]
    }
   ],
   "source": [
    "win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max = predict_simu(lsvm, \n",
    "                                                                                                          X_test_simu_n, \n",
    "                                                                                                          y_test, \n",
    "                                                                                                          500)\n",
    "print('Total Return: {0}%'.format(round(total_return,0)))\n",
    "print('Final Capital: ${0}'.format(round(capital,0)))\n",
    "print('Win Count: ',win_count)\n",
    "print('Lost Count: ',lost_count)\n",
    "print('No Order Count: ', no_order_count)\n",
    "print('Max Consecutive Lost: {0} trades'.format(consecutive_lost_max/-3.3))\n",
    "day_one = X_test_simu_n.iloc[0].name.to_pydatetime().date()\n",
    "day_final = X_test_simu_n.iloc[len(X_test_simu_n)-1].name.to_pydatetime().date()\n",
    "print('Total day on trading: {0} days'.format((day_final - day_one).days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_df.plot(y='record_capital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version should be 4.0: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('version should be 4.0:',pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsvm,open('lsvm_xauusd','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('lsvm_xauusd','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = [-2.839808,-2.806494,-2.828501,-2.791640]\n",
    "model_input = np.array(model_input).reshape(1,-1)\n",
    "model_input\n",
    "pickled_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Creation of API before putting in Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "xa = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "output 1\n"
     ]
    }
   ],
   "source": [
    "from flask import jsonify\n",
    "import json\n",
    "\n",
    "methods = {\n",
    "\t\"data\": {\n",
    "\t\t\"open\": 1910,\n",
    "\t\t\"high\": 1920,\n",
    "\t\t\"low\": 1900,\n",
    "\t\t\"close\": 1915\n",
    "\t}\n",
    "}\n",
    "\n",
    "# getting to request in json\n",
    "data = json.dumps(methods['data'])\n",
    "\n",
    "# making into dict capture in \"data\"\n",
    "data = json.loads(data)\n",
    "print(type(data))\n",
    "\n",
    "new_data = scaler.transform(np.array(list(data.values())).reshape(1,-1))\n",
    "output = lsvm.predict(new_data)\n",
    "output = int(output[0])\n",
    "print('output', output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question on SVM\n",
    "\n",
    "##### what is the different between svm.SVC(kernel='linear') and LinearSVC in sklearn?\n",
    "\n",
    "svm.SVC(kernel='linear') and LinearSVC are both linear SVM classifiers available in scikit-learn, but there are some differences between them.\n",
    "\n",
    "svm.SVC(kernel='linear') is based on the LIBSVM library, which is written in C++, while LinearSVC is based on the LIBLINEAR library, which is written in C.\n",
    "\n",
    "One of the main differences between these two is the handling of multi-class classification. svm.SVC(kernel='linear') uses the one-vs-one approach, which trains a binary classifier for every pair of classes, while LinearSVC uses the one-vs-the-rest approach, which trains a binary classifier for each class against all the other classes.\n",
    "\n",
    "Another difference is the way they handle the regularization parameter C. In svm.SVC(kernel='linear'), C controls the tradeoff between maximizing the margin and minimizing the classification error. In LinearSVC, C controls the strength of regularization, where smaller values of C lead to stronger regularization.\n",
    "\n",
    "Overall, LinearSVC is faster and more memory-efficient than svm.SVC(kernel='linear') and is recommended for large datasets with many features, while svm.SVC(kernel='linear') may be more suitable for smaller datasets with few features or for multi-class classification.\n",
    "\n",
    "##### which is created first\n",
    "\n",
    "SVC(kernel='linear') was created first and it is a more flexible implementation of linear support vector machines. It can handle non-linearly separable data by using the kernel trick to transform the input data into a higher-dimensional feature space where the data becomes linearly separable.\n",
    "\n",
    "LinearSVC, on the other hand, is a more efficient implementation of linear support vector machines that only supports linear kernels. It uses a different algorithm for training the model that is more scalable and faster than the algorithm used by SVC(kernel='linear').\n",
    "\n",
    "Both models are useful for solving binary classification problems, but LinearSVC is generally preferred when the dataset is large and the number of features is high, while SVC(kernel='linear') is preferred when the dataset is smaller or when non-linearly separable data is involved.\n",
    "\n",
    "##### Definition of large dataset and high number of features\n",
    "\n",
    "The definitions of \"large dataset\" and \"high number of features\" are somewhat relative and can vary depending on the context of the problem and the available computational resources. However, as a general rule of thumb, a \"large dataset\" can refer to a dataset with tens of thousands or more samples, while a \"small dataset\" can refer to a dataset with a few hundred or a few thousand samples.\n",
    "\n",
    "Similarly, a \"high number of features\" can also be relative, but it generally refers to datasets where the number of features is comparable to or larger than the number of samples. For example, in genomics and bioinformatics, it is common to work with datasets with tens of thousands or even hundreds of thousands of features.\n",
    "\n",
    "In practice, when working with a large dataset with a high number of features, LinearSVC can be more computationally efficient and can handle such datasets more effectively. On the other hand, when working with a smaller dataset with a lower number of features, SVC(kernel='linear') can perform just as well and may offer better accuracy in some cases.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question XGBoost\n",
    "\n",
    "Random Forest and XGBoost are both popular machine learning algorithms used for regression and classification tasks. However, there are several key differences between these algorithms, which are outlined below:\n",
    "\n",
    "**During the training process**, XGBoost uses gradient boosting to optimize a loss function. The loss function measures the difference between the predicted and actual values and is chosen based on the specific problem being solved. For example, the mean squared error (MSE) loss function can be used for regression problems, and the cross-entropy loss function can be used for classification problems.\n",
    "\n",
    "**Model architecture:** Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to make a final prediction. Each decision tree in the Random Forest is built independently and does not depend on the other trees. On the other hand, XGBoost is a boosted tree algorithm that builds decision trees sequentially, where each new tree corrects the errors of the previous trees.\n",
    "\n",
    "**Tree construction:** In Random Forest, each decision tree is built by randomly selecting a subset of the features and the samples in the training set. This helps to reduce overfitting and improves the performance of the model. In XGBoost, each tree is built by greedily selecting the best split that maximizes the information gain.\n",
    "\n",
    "**Handling of missing data:** Random Forest can handle missing data by imputing the missing values with the mean or median of the feature. XGBoost can handle missing data by splitting the samples into two groups: one group with the missing value and one group without the missing value.\n",
    "\n",
    "**Regularization:** Random Forest does not have any regularization parameters. XGBoost has several regularization parameters, including the learning rate, which controls the step size during the gradient descent, and the regularization term, which penalizes complex models and helps to prevent overfitting.\n",
    "\n",
    "**Performance:** Random Forest is generally faster to train than XGBoost, especially for large datasets. However, XGBoost often outperforms Random Forest in terms of predictive accuracy, especially for complex tasks with high-dimensional features.\n",
    "\n",
    "**In summary,** Random Forest and XGBoost are both powerful machine learning algorithms with different strengths and weaknesses. Random Forest is a simple and fast algorithm that can handle missing data, while XGBoost is a more complex algorithm that can handle complex tasks and has better predictive accuracy, but may require more tuning and training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

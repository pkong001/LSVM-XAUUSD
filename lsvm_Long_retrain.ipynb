{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2 id=\"load_dataset\">Load the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Here we will load from github or do API or something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load Data From CSV File\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Welcome to Alpha Vantage! Here is your API key: 9NNSMISKUYVUTO0M. Please record this API key at a safe place for future data access.</span>\n",
    "from tvDatafeed import TvDatafeed, Interval\n",
    "\n",
    "username = 'pkongdan01'\n",
    "password = 'Secu@3545'\n",
    "\n",
    "tv = TvDatafeed(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    sec      open      high       low     close  volume\n",
      "0  2023.01.02  18:00  1826.837  1827.337  1826.617  1826.637       0\n",
      "1  2023.01.02  18:01  1826.537  1827.357  1826.137  1826.537       0\n",
      "2  2023.01.02  18:02  1826.137  1826.737  1826.137  1826.737       0\n",
      "3  2023.01.02  18:05  1827.187  1828.867  1827.187  1828.738       0\n",
      "4  2023.01.02  18:06  1828.758  1829.958  1828.758  1829.497       0\n",
      "date      0\n",
      "sec       0\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "date       object\n",
      "sec        object\n",
      "open      float64\n",
      "high      float64\n",
      "low       float64\n",
      "close     float64\n",
      "volume      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "xau23 = pd.read_csv(\"./data/xau23.csv\", header=None)\n",
    "xau22 = pd.read_csv(\"./data/xau22.csv\", header=None)\n",
    "xau21 = pd.read_csv(\"./data/xau21.csv\", header=None)\n",
    "\n",
    "# create a columns name\n",
    "cname= [\"date\",\"sec\",\"open\", \"high\", \"low\", \"close\",\"volume\"]\n",
    "xau23.columns = cname\n",
    "xau22.columns = cname\n",
    "xau21.columns = cname\n",
    "\n",
    "# union the data set \n",
    "df_import = pd.concat([xau23,xau22,xau21])\n",
    "print(df_import.head())\n",
    "\n",
    "# Check for N/A\n",
    "print(df_import.isna().sum())\n",
    "# Check for types\n",
    "print(df_import.dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime    datetime64[ns]\n",
      "open               float64\n",
      "high               float64\n",
      "low                float64\n",
      "close              float64\n",
      "dtype: object\n",
      "open     6170\n",
      "high     6170\n",
      "low      6170\n",
      "close    6170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# combine the date and time columns into a single string\n",
    "df_import['datetime'] = df_import['date'] + df_import['sec']\n",
    "\n",
    "# convert the combined string to a Pandas datetime object\n",
    "df_import['datetime'] = pd.to_datetime(df_import['datetime'], format = '%Y.%m.%d%H:%M')\n",
    "\n",
    "# reselect the data\n",
    "df_import = df_import[['datetime','open','high','low','close']]\n",
    "\n",
    "# sort the data ascending\n",
    "df_import = df_import.sort_values(by='datetime',ascending=True)\n",
    "\n",
    "print(df_import.dtypes)\n",
    "\n",
    "# set the datetime column as the index\n",
    "df_import.set_index('datetime', inplace=True)\n",
    "\n",
    "# resample data into 1hour interval\n",
    "df_import = df_import.resample('1H').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'})\n",
    "print(df_import.isnull().sum())\n",
    "\n",
    "# drop null value (Market Close on those days)\n",
    "df_import = df_import.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features (lagged candles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>open_lag1</th>\n",
       "      <th>open_lag2</th>\n",
       "      <th>open_lag3</th>\n",
       "      <th>open_lag4</th>\n",
       "      <th>open_lag5</th>\n",
       "      <th>high_lag1</th>\n",
       "      <th>...</th>\n",
       "      <th>low_lag1</th>\n",
       "      <th>low_lag2</th>\n",
       "      <th>low_lag3</th>\n",
       "      <th>low_lag4</th>\n",
       "      <th>low_lag5</th>\n",
       "      <th>close_lag1</th>\n",
       "      <th>close_lag2</th>\n",
       "      <th>close_lag3</th>\n",
       "      <th>close_lag4</th>\n",
       "      <th>close_lag5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-03 23:00:00</th>\n",
       "      <td>1923.119</td>\n",
       "      <td>1923.634</td>\n",
       "      <td>1920.108</td>\n",
       "      <td>1922.605</td>\n",
       "      <td>1921.695</td>\n",
       "      <td>1916.558</td>\n",
       "      <td>1913.278</td>\n",
       "      <td>1913.025</td>\n",
       "      <td>1904.998</td>\n",
       "      <td>1925.145</td>\n",
       "      <td>...</td>\n",
       "      <td>1920.675</td>\n",
       "      <td>1915.394</td>\n",
       "      <td>1912.124</td>\n",
       "      <td>1909.858</td>\n",
       "      <td>1903.288</td>\n",
       "      <td>1923.120</td>\n",
       "      <td>1921.675</td>\n",
       "      <td>1916.524</td>\n",
       "      <td>1913.278</td>\n",
       "      <td>1913.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 00:00:00</th>\n",
       "      <td>1922.595</td>\n",
       "      <td>1922.925</td>\n",
       "      <td>1918.984</td>\n",
       "      <td>1921.548</td>\n",
       "      <td>1923.119</td>\n",
       "      <td>1921.695</td>\n",
       "      <td>1916.558</td>\n",
       "      <td>1913.278</td>\n",
       "      <td>1913.025</td>\n",
       "      <td>1923.634</td>\n",
       "      <td>...</td>\n",
       "      <td>1920.108</td>\n",
       "      <td>1920.675</td>\n",
       "      <td>1915.394</td>\n",
       "      <td>1912.124</td>\n",
       "      <td>1909.858</td>\n",
       "      <td>1922.605</td>\n",
       "      <td>1923.120</td>\n",
       "      <td>1921.675</td>\n",
       "      <td>1916.524</td>\n",
       "      <td>1913.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 01:00:00</th>\n",
       "      <td>1921.648</td>\n",
       "      <td>1925.475</td>\n",
       "      <td>1921.345</td>\n",
       "      <td>1922.835</td>\n",
       "      <td>1922.595</td>\n",
       "      <td>1923.119</td>\n",
       "      <td>1921.695</td>\n",
       "      <td>1916.558</td>\n",
       "      <td>1913.278</td>\n",
       "      <td>1922.925</td>\n",
       "      <td>...</td>\n",
       "      <td>1918.984</td>\n",
       "      <td>1920.108</td>\n",
       "      <td>1920.675</td>\n",
       "      <td>1915.394</td>\n",
       "      <td>1912.124</td>\n",
       "      <td>1921.548</td>\n",
       "      <td>1922.605</td>\n",
       "      <td>1923.120</td>\n",
       "      <td>1921.675</td>\n",
       "      <td>1916.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 02:00:00</th>\n",
       "      <td>1922.828</td>\n",
       "      <td>1925.428</td>\n",
       "      <td>1922.318</td>\n",
       "      <td>1924.668</td>\n",
       "      <td>1921.648</td>\n",
       "      <td>1922.595</td>\n",
       "      <td>1923.119</td>\n",
       "      <td>1921.695</td>\n",
       "      <td>1916.558</td>\n",
       "      <td>1925.475</td>\n",
       "      <td>...</td>\n",
       "      <td>1921.345</td>\n",
       "      <td>1918.984</td>\n",
       "      <td>1920.108</td>\n",
       "      <td>1920.675</td>\n",
       "      <td>1915.394</td>\n",
       "      <td>1922.835</td>\n",
       "      <td>1921.548</td>\n",
       "      <td>1922.605</td>\n",
       "      <td>1923.120</td>\n",
       "      <td>1921.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 03:00:00</th>\n",
       "      <td>1924.678</td>\n",
       "      <td>1935.085</td>\n",
       "      <td>1924.468</td>\n",
       "      <td>1932.038</td>\n",
       "      <td>1922.828</td>\n",
       "      <td>1921.648</td>\n",
       "      <td>1922.595</td>\n",
       "      <td>1923.119</td>\n",
       "      <td>1921.695</td>\n",
       "      <td>1925.428</td>\n",
       "      <td>...</td>\n",
       "      <td>1922.318</td>\n",
       "      <td>1921.345</td>\n",
       "      <td>1918.984</td>\n",
       "      <td>1920.108</td>\n",
       "      <td>1920.675</td>\n",
       "      <td>1924.668</td>\n",
       "      <td>1922.835</td>\n",
       "      <td>1921.548</td>\n",
       "      <td>1922.605</td>\n",
       "      <td>1923.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 19:00:00</th>\n",
       "      <td>1825.374</td>\n",
       "      <td>1825.565</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1827.218</td>\n",
       "      <td>...</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1825.316</td>\n",
       "      <td>1827.848</td>\n",
       "      <td>1826.255</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1823.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 20:00:00</th>\n",
       "      <td>1823.365</td>\n",
       "      <td>1829.425</td>\n",
       "      <td>1823.105</td>\n",
       "      <td>1829.005</td>\n",
       "      <td>1825.374</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1825.565</td>\n",
       "      <td>...</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1825.316</td>\n",
       "      <td>1827.848</td>\n",
       "      <td>1826.255</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1829.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 21:00:00</th>\n",
       "      <td>1829.045</td>\n",
       "      <td>1829.195</td>\n",
       "      <td>1827.305</td>\n",
       "      <td>1827.635</td>\n",
       "      <td>1823.365</td>\n",
       "      <td>1825.374</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1829.425</td>\n",
       "      <td>...</td>\n",
       "      <td>1823.105</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1825.316</td>\n",
       "      <td>1827.848</td>\n",
       "      <td>1829.005</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1828.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 22:00:00</th>\n",
       "      <td>1827.625</td>\n",
       "      <td>1831.148</td>\n",
       "      <td>1827.405</td>\n",
       "      <td>1830.895</td>\n",
       "      <td>1829.045</td>\n",
       "      <td>1823.365</td>\n",
       "      <td>1825.374</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1829.195</td>\n",
       "      <td>...</td>\n",
       "      <td>1827.305</td>\n",
       "      <td>1823.105</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1825.316</td>\n",
       "      <td>1827.635</td>\n",
       "      <td>1829.005</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 23:00:00</th>\n",
       "      <td>1830.895</td>\n",
       "      <td>1831.585</td>\n",
       "      <td>1829.845</td>\n",
       "      <td>1830.015</td>\n",
       "      <td>1827.625</td>\n",
       "      <td>1829.045</td>\n",
       "      <td>1823.365</td>\n",
       "      <td>1825.374</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1831.148</td>\n",
       "      <td>...</td>\n",
       "      <td>1827.405</td>\n",
       "      <td>1827.305</td>\n",
       "      <td>1823.105</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1830.895</td>\n",
       "      <td>1827.635</td>\n",
       "      <td>1829.005</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1825.395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12695 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close  open_lag1  \\\n",
       "datetime                                                                 \n",
       "2021-01-03 23:00:00  1923.119  1923.634  1920.108  1922.605   1921.695   \n",
       "2021-01-04 00:00:00  1922.595  1922.925  1918.984  1921.548   1923.119   \n",
       "2021-01-04 01:00:00  1921.648  1925.475  1921.345  1922.835   1922.595   \n",
       "2021-01-04 02:00:00  1922.828  1925.428  1922.318  1924.668   1921.648   \n",
       "2021-01-04 03:00:00  1924.678  1935.085  1924.468  1932.038   1922.828   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2023-02-28 19:00:00  1825.374  1825.565  1822.938  1823.355   1826.104   \n",
       "2023-02-28 20:00:00  1823.365  1829.425  1823.105  1829.005   1825.374   \n",
       "2023-02-28 21:00:00  1829.045  1829.195  1827.305  1827.635   1823.365   \n",
       "2023-02-28 22:00:00  1827.625  1831.148  1827.405  1830.895   1829.045   \n",
       "2023-02-28 23:00:00  1830.895  1831.585  1829.845  1830.015   1827.625   \n",
       "\n",
       "                     open_lag2  open_lag3  open_lag4  open_lag5  high_lag1  \\\n",
       "datetime                                                                     \n",
       "2021-01-03 23:00:00   1916.558   1913.278   1913.025   1904.998   1925.145   \n",
       "2021-01-04 00:00:00   1921.695   1916.558   1913.278   1913.025   1923.634   \n",
       "2021-01-04 01:00:00   1923.119   1921.695   1916.558   1913.278   1922.925   \n",
       "2021-01-04 02:00:00   1922.595   1923.119   1921.695   1916.558   1925.475   \n",
       "2021-01-04 03:00:00   1921.648   1922.595   1923.119   1921.695   1925.428   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2023-02-28 19:00:00   1826.375   1827.978   1826.518   1818.608   1827.218   \n",
       "2023-02-28 20:00:00   1826.104   1826.375   1827.978   1826.518   1825.565   \n",
       "2023-02-28 21:00:00   1825.374   1826.104   1826.375   1827.978   1829.425   \n",
       "2023-02-28 22:00:00   1823.365   1825.374   1826.104   1826.375   1829.195   \n",
       "2023-02-28 23:00:00   1829.045   1823.365   1825.374   1826.104   1831.148   \n",
       "\n",
       "                     ...  low_lag1  low_lag2  low_lag3  low_lag4  low_lag5  \\\n",
       "datetime             ...                                                     \n",
       "2021-01-03 23:00:00  ...  1920.675  1915.394  1912.124  1909.858  1903.288   \n",
       "2021-01-04 00:00:00  ...  1920.108  1920.675  1915.394  1912.124  1909.858   \n",
       "2021-01-04 01:00:00  ...  1918.984  1920.108  1920.675  1915.394  1912.124   \n",
       "2021-01-04 02:00:00  ...  1921.345  1918.984  1920.108  1920.675  1915.394   \n",
       "2021-01-04 03:00:00  ...  1922.318  1921.345  1918.984  1920.108  1920.675   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "2023-02-28 19:00:00  ...  1825.265  1825.316  1827.848  1826.255  1818.608   \n",
       "2023-02-28 20:00:00  ...  1822.938  1825.265  1825.316  1827.848  1826.255   \n",
       "2023-02-28 21:00:00  ...  1823.105  1822.938  1825.265  1825.316  1827.848   \n",
       "2023-02-28 22:00:00  ...  1827.305  1823.105  1822.938  1825.265  1825.316   \n",
       "2023-02-28 23:00:00  ...  1827.405  1827.305  1823.105  1822.938  1825.265   \n",
       "\n",
       "                     close_lag1  close_lag2  close_lag3  close_lag4  \\\n",
       "datetime                                                              \n",
       "2021-01-03 23:00:00    1923.120    1921.675    1916.524    1913.278   \n",
       "2021-01-04 00:00:00    1922.605    1923.120    1921.675    1916.524   \n",
       "2021-01-04 01:00:00    1921.548    1922.605    1923.120    1921.675   \n",
       "2021-01-04 02:00:00    1922.835    1921.548    1922.605    1923.120   \n",
       "2021-01-04 03:00:00    1924.668    1922.835    1921.548    1922.605   \n",
       "...                         ...         ...         ...         ...   \n",
       "2023-02-28 19:00:00    1825.395    1826.648    1828.915    1829.168   \n",
       "2023-02-28 20:00:00    1823.355    1825.395    1826.648    1828.915   \n",
       "2023-02-28 21:00:00    1829.005    1823.355    1825.395    1826.648   \n",
       "2023-02-28 22:00:00    1827.635    1829.005    1823.355    1825.395   \n",
       "2023-02-28 23:00:00    1830.895    1827.635    1829.005    1823.355   \n",
       "\n",
       "                     close_lag5  \n",
       "datetime                         \n",
       "2021-01-03 23:00:00    1913.035  \n",
       "2021-01-04 00:00:00    1913.278  \n",
       "2021-01-04 01:00:00    1916.524  \n",
       "2021-01-04 02:00:00    1921.675  \n",
       "2021-01-04 03:00:00    1923.120  \n",
       "...                         ...  \n",
       "2023-02-28 19:00:00    1823.975  \n",
       "2023-02-28 20:00:00    1829.168  \n",
       "2023-02-28 21:00:00    1828.915  \n",
       "2023-02-28 22:00:00    1826.648  \n",
       "2023-02-28 23:00:00    1825.395  \n",
       "\n",
       "[12695 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lags = 5\n",
    "for col in df_import.columns:\n",
    "    for i in range(1, num_lags+1):\n",
    "        col_name = col + '_lag' + str(i)\n",
    "        df_import[col_name] = df_import[col].shift(i)\n",
    "df_import = df_import.dropna()\n",
    "df_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features (indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         open      high       low     close  open_lag1  \\\n",
      "datetime                                                                 \n",
      "2021-01-06 11:00:00  1909.649  1909.848  1900.755  1907.965   1932.345   \n",
      "2021-01-06 12:00:00  1907.978  1908.028  1902.448  1907.478   1909.649   \n",
      "2021-01-06 13:00:00  1907.478  1911.348  1905.968  1911.248   1907.978   \n",
      "2021-01-06 14:00:00  1911.158  1921.208  1908.488  1918.365   1907.478   \n",
      "2021-01-06 15:00:00  1918.305  1924.175  1915.208  1918.088   1911.158   \n",
      "\n",
      "                     open_lag2  open_lag3  open_lag4  open_lag5  high_lag1  \\\n",
      "datetime                                                                     \n",
      "2021-01-06 11:00:00   1932.825   1933.809   1945.168   1956.595   1933.658   \n",
      "2021-01-06 12:00:00   1932.345   1932.825   1933.809   1945.168   1909.848   \n",
      "2021-01-06 13:00:00   1909.649   1932.345   1932.825   1933.809   1908.028   \n",
      "2021-01-06 14:00:00   1907.978   1909.649   1932.345   1932.825   1911.348   \n",
      "2021-01-06 15:00:00   1907.478   1907.978   1909.649   1932.345   1921.208   \n",
      "\n",
      "                     ...  low_lag3  low_lag4  low_lag5  close_lag1  \\\n",
      "datetime             ...                                             \n",
      "2021-01-06 11:00:00  ...  1930.588  1924.989  1945.168    1909.728   \n",
      "2021-01-06 12:00:00  ...  1927.288  1930.588  1924.989    1907.965   \n",
      "2021-01-06 13:00:00  ...  1903.895  1927.288  1930.588    1907.478   \n",
      "2021-01-06 14:00:00  ...  1900.755  1903.895  1927.288    1911.248   \n",
      "2021-01-06 15:00:00  ...  1902.448  1900.755  1903.895    1918.365   \n",
      "\n",
      "                     close_lag2  close_lag3  close_lag4  close_lag5  \\\n",
      "datetime                                                              \n",
      "2021-01-06 11:00:00    1932.395    1932.858    1933.849    1945.168   \n",
      "2021-01-06 12:00:00    1909.728    1932.395    1932.858    1933.849   \n",
      "2021-01-06 13:00:00    1907.965    1909.728    1932.395    1932.858   \n",
      "2021-01-06 14:00:00    1907.478    1907.965    1909.728    1932.395   \n",
      "2021-01-06 15:00:00    1911.248    1907.478    1907.965    1909.728   \n",
      "\n",
      "                            EMAF      STO_K  \n",
      "datetime                                     \n",
      "2021-01-06 11:00:00  1940.718746  15.858660  \n",
      "2021-01-06 12:00:00  1939.610721  13.895802  \n",
      "2021-01-06 13:00:00  1938.665297  19.664488  \n",
      "2021-01-06 14:00:00  1937.988620  32.465316  \n",
      "2021-01-06 15:00:00  1937.325266  51.887239  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df_import.copy()\n",
    "# print(xau_data.isnull().sum())\n",
    "# df['RSI'] = ta.rsi(df.close, length=15)\n",
    "df['EMAF'] = ta.ema(df.close, length = 59)\n",
    "# df['EMAM'] = ta.ema(df.close, length = 100)\n",
    "# df['EMAS'] = ta.ema(df.close, length = 150)\n",
    "k=5\n",
    "stoch = ta.stoch(high=df['high'], low=df['low'], close=df['close'], k=k, d=3, smooth_k=3)\n",
    "df['STO_K'] = stoch[f'STOCHk_{k}_3_3']\n",
    "#df['STO_D'] = stoch[f'STOCHd_{k}_3_3']\n",
    "\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"./csv/indicator_check.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Condition for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5019\n",
      "0 7614\n",
      "check null value open               0\n",
      "high               0\n",
      "low                0\n",
      "close              0\n",
      "open_lag1          0\n",
      "open_lag2          0\n",
      "open_lag3          0\n",
      "open_lag4          0\n",
      "open_lag5          0\n",
      "high_lag1          0\n",
      "high_lag2          0\n",
      "high_lag3          0\n",
      "high_lag4          0\n",
      "high_lag5          0\n",
      "low_lag1           0\n",
      "low_lag2           0\n",
      "low_lag3           0\n",
      "low_lag4           0\n",
      "low_lag5           0\n",
      "close_lag1         0\n",
      "close_lag2         0\n",
      "close_lag3         0\n",
      "close_lag4         0\n",
      "close_lag5         0\n",
      "EMAF               0\n",
      "STO_K              0\n",
      "high_close_diff    0\n",
      "low_close_diff     0\n",
      "target             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "highs = df['high'].rolling(window=4).max().shift(-4)\n",
    "lows = df['low'].rolling(window=4).max().shift(-4)\n",
    "# create new columns for conditions \n",
    "df['high_close_diff'] = highs - df['close']\n",
    "df['low_close_diff'] = lows - df['close']\n",
    "\n",
    "def reco(row):\n",
    "    if row.low_close_diff <= -3.3:\n",
    "        return 0\n",
    "    elif row.high_close_diff >= 4.3: #I use 3 plus .3 for bit offer off-set, and 1 in case of delay ordering cause price to change.\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['target'] = df.apply(reco,axis=1)\n",
    "# df[\"mhigh\"]=highs\n",
    "# df[\"mlow\"]=lows\n",
    "df = df.dropna()\n",
    "\n",
    "df_simu = df\n",
    "print('1', (df['target'] == 1).sum())\n",
    "print('0', (df['target'] == 0).sum())\n",
    "print(f\"check null value {df.isnull().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Training, Cross Validation, and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12633\n",
      "Training set start: 2021-01-06 11:00:00 \n",
      " Training set end: 2022-07-06 09:00:00\n",
      "cross validation set start: 2022-07-06 10:00:00 \n",
      " cross validation set end: 2022-10-30 20:00:00\n",
      "test set start: 2022-10-30 21:00:00 \n",
      " test set end: 2023-02-28 19:00:00\n",
      "Shape of X_train: (8844, 26)\n",
      "Shape of y_train: (8844,)\n",
      "Shape of X_cross: (1895, 26)\n",
      "Shape of y_cross: (1895,)\n",
      "Shape of X_test: (1894, 26)\n",
      "Shape of y_test: (1894,)\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(n)\n",
    "\n",
    "train_n = int(round(n*0.70,0))\n",
    "cross_n = int(round(n*0.15,0))\n",
    "test_n = int(round(n*0.15,0))\n",
    "\n",
    "\n",
    "train_start = str(df.iloc[0].name)\n",
    "train_end = str(df.iloc[train_n].name)\n",
    "print('Training set start: {0} \\n Training set end: {1}'.format(train_start,train_end))\n",
    "#print(df.loc[train_start:train_end])\n",
    "\n",
    "cross_start = str(df.iloc[train_n+1].name)\n",
    "cross_end = str(df.iloc[train_n+cross_n].name)\n",
    "print('cross validation set start: {0} \\n cross validation set end: {1}'.format(cross_start,cross_end))\n",
    "#print(df.loc[cross_start:cross_end])\n",
    "\n",
    "test_start = str(df.iloc[train_n + cross_n +1].name)\n",
    "test_end = str(df.iloc[n-1].name)\n",
    "print('test set start: {0} \\n test set end: {1}'.format(test_start,test_end))\n",
    "#print(df.loc[test_start:])\n",
    "\n",
    "X = df.iloc[:,:len(df.columns)-3]\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train = np.asarray(X.loc[train_start:train_end])\n",
    "y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "X_test = np.asarray(X.loc[test_start:])\n",
    "y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "\n",
    "#Extra for GridSearch\n",
    "X_train_cv = np.asarray(X.loc[train_start:cross_end])\n",
    "y_train_cv = np.asarray(y.loc[train_start:cross_end])\n",
    "\n",
    "\n",
    "print(\"Shape of X_train:\", np.shape(X_train))\n",
    "print(\"Shape of y_train:\", np.shape(y_train))\n",
    "print(\"Shape of X_cross:\", np.shape(X_cross))\n",
    "print(\"Shape of y_cross:\", np.shape(y_cross))\n",
    "print(\"Shape of X_test:\", np.shape(X_test))\n",
    "print(\"Shape of y_test:\", np.shape(y_test))\n",
    "\n",
    "\n",
    "# Below is for later simulation\n",
    "df_simu_test = df_simu.loc[test_start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>open_lag1</th>\n",
       "      <th>open_lag2</th>\n",
       "      <th>open_lag3</th>\n",
       "      <th>open_lag4</th>\n",
       "      <th>open_lag5</th>\n",
       "      <th>high_lag1</th>\n",
       "      <th>...</th>\n",
       "      <th>low_lag3</th>\n",
       "      <th>low_lag4</th>\n",
       "      <th>low_lag5</th>\n",
       "      <th>close_lag1</th>\n",
       "      <th>close_lag2</th>\n",
       "      <th>close_lag3</th>\n",
       "      <th>close_lag4</th>\n",
       "      <th>close_lag5</th>\n",
       "      <th>EMAF</th>\n",
       "      <th>STO_K</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-06 11:00:00</th>\n",
       "      <td>1909.649</td>\n",
       "      <td>1909.848</td>\n",
       "      <td>1900.755</td>\n",
       "      <td>1907.965</td>\n",
       "      <td>1932.345</td>\n",
       "      <td>1932.825</td>\n",
       "      <td>1933.809</td>\n",
       "      <td>1945.168</td>\n",
       "      <td>1956.595</td>\n",
       "      <td>1933.658</td>\n",
       "      <td>...</td>\n",
       "      <td>1930.588</td>\n",
       "      <td>1924.989</td>\n",
       "      <td>1945.168</td>\n",
       "      <td>1909.728</td>\n",
       "      <td>1932.395</td>\n",
       "      <td>1932.858</td>\n",
       "      <td>1933.849</td>\n",
       "      <td>1945.168</td>\n",
       "      <td>1940.718746</td>\n",
       "      <td>15.858660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 12:00:00</th>\n",
       "      <td>1907.978</td>\n",
       "      <td>1908.028</td>\n",
       "      <td>1902.448</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1909.649</td>\n",
       "      <td>1932.345</td>\n",
       "      <td>1932.825</td>\n",
       "      <td>1933.809</td>\n",
       "      <td>1945.168</td>\n",
       "      <td>1909.848</td>\n",
       "      <td>...</td>\n",
       "      <td>1927.288</td>\n",
       "      <td>1930.588</td>\n",
       "      <td>1924.989</td>\n",
       "      <td>1907.965</td>\n",
       "      <td>1909.728</td>\n",
       "      <td>1932.395</td>\n",
       "      <td>1932.858</td>\n",
       "      <td>1933.849</td>\n",
       "      <td>1939.610721</td>\n",
       "      <td>13.895802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 13:00:00</th>\n",
       "      <td>1907.478</td>\n",
       "      <td>1911.348</td>\n",
       "      <td>1905.968</td>\n",
       "      <td>1911.248</td>\n",
       "      <td>1907.978</td>\n",
       "      <td>1909.649</td>\n",
       "      <td>1932.345</td>\n",
       "      <td>1932.825</td>\n",
       "      <td>1933.809</td>\n",
       "      <td>1908.028</td>\n",
       "      <td>...</td>\n",
       "      <td>1903.895</td>\n",
       "      <td>1927.288</td>\n",
       "      <td>1930.588</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1907.965</td>\n",
       "      <td>1909.728</td>\n",
       "      <td>1932.395</td>\n",
       "      <td>1932.858</td>\n",
       "      <td>1938.665297</td>\n",
       "      <td>19.664488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 14:00:00</th>\n",
       "      <td>1911.158</td>\n",
       "      <td>1921.208</td>\n",
       "      <td>1908.488</td>\n",
       "      <td>1918.365</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1907.978</td>\n",
       "      <td>1909.649</td>\n",
       "      <td>1932.345</td>\n",
       "      <td>1932.825</td>\n",
       "      <td>1911.348</td>\n",
       "      <td>...</td>\n",
       "      <td>1900.755</td>\n",
       "      <td>1903.895</td>\n",
       "      <td>1927.288</td>\n",
       "      <td>1911.248</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1907.965</td>\n",
       "      <td>1909.728</td>\n",
       "      <td>1932.395</td>\n",
       "      <td>1937.988620</td>\n",
       "      <td>32.465316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 15:00:00</th>\n",
       "      <td>1918.305</td>\n",
       "      <td>1924.175</td>\n",
       "      <td>1915.208</td>\n",
       "      <td>1918.088</td>\n",
       "      <td>1911.158</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1907.978</td>\n",
       "      <td>1909.649</td>\n",
       "      <td>1932.345</td>\n",
       "      <td>1921.208</td>\n",
       "      <td>...</td>\n",
       "      <td>1902.448</td>\n",
       "      <td>1900.755</td>\n",
       "      <td>1903.895</td>\n",
       "      <td>1918.365</td>\n",
       "      <td>1911.248</td>\n",
       "      <td>1907.478</td>\n",
       "      <td>1907.965</td>\n",
       "      <td>1909.728</td>\n",
       "      <td>1937.325266</td>\n",
       "      <td>51.887239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 12:00:00</th>\n",
       "      <td>1826.518</td>\n",
       "      <td>1831.045</td>\n",
       "      <td>1826.255</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1809.258</td>\n",
       "      <td>1808.915</td>\n",
       "      <td>1810.575</td>\n",
       "      <td>1810.425</td>\n",
       "      <td>1826.645</td>\n",
       "      <td>...</td>\n",
       "      <td>1807.745</td>\n",
       "      <td>1809.414</td>\n",
       "      <td>1804.594</td>\n",
       "      <td>1823.975</td>\n",
       "      <td>1813.804</td>\n",
       "      <td>1808.975</td>\n",
       "      <td>1810.235</td>\n",
       "      <td>1810.565</td>\n",
       "      <td>1818.791606</td>\n",
       "      <td>87.624677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 14:00:00</th>\n",
       "      <td>1827.978</td>\n",
       "      <td>1830.218</td>\n",
       "      <td>1827.848</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1809.258</td>\n",
       "      <td>1808.915</td>\n",
       "      <td>1810.575</td>\n",
       "      <td>1831.045</td>\n",
       "      <td>...</td>\n",
       "      <td>1807.108</td>\n",
       "      <td>1807.745</td>\n",
       "      <td>1809.414</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1823.975</td>\n",
       "      <td>1813.804</td>\n",
       "      <td>1808.975</td>\n",
       "      <td>1810.235</td>\n",
       "      <td>1819.129052</td>\n",
       "      <td>90.383977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 16:00:00</th>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.345</td>\n",
       "      <td>1825.316</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1809.258</td>\n",
       "      <td>1808.915</td>\n",
       "      <td>1830.218</td>\n",
       "      <td>...</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1807.108</td>\n",
       "      <td>1807.745</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1823.975</td>\n",
       "      <td>1813.804</td>\n",
       "      <td>1808.975</td>\n",
       "      <td>1819.379684</td>\n",
       "      <td>88.297058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 18:00:00</th>\n",
       "      <td>1826.104</td>\n",
       "      <td>1827.218</td>\n",
       "      <td>1825.265</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1809.258</td>\n",
       "      <td>1827.345</td>\n",
       "      <td>...</td>\n",
       "      <td>1826.255</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1807.108</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1823.975</td>\n",
       "      <td>1813.804</td>\n",
       "      <td>1819.580194</td>\n",
       "      <td>75.767876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28 19:00:00</th>\n",
       "      <td>1825.374</td>\n",
       "      <td>1825.565</td>\n",
       "      <td>1822.938</td>\n",
       "      <td>1823.355</td>\n",
       "      <td>1826.104</td>\n",
       "      <td>1826.375</td>\n",
       "      <td>1827.978</td>\n",
       "      <td>1826.518</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1827.218</td>\n",
       "      <td>...</td>\n",
       "      <td>1827.848</td>\n",
       "      <td>1826.255</td>\n",
       "      <td>1818.608</td>\n",
       "      <td>1825.395</td>\n",
       "      <td>1826.648</td>\n",
       "      <td>1828.915</td>\n",
       "      <td>1829.168</td>\n",
       "      <td>1823.975</td>\n",
       "      <td>1819.706021</td>\n",
       "      <td>47.115230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12633 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close  open_lag1  \\\n",
       "datetime                                                                 \n",
       "2021-01-06 11:00:00  1909.649  1909.848  1900.755  1907.965   1932.345   \n",
       "2021-01-06 12:00:00  1907.978  1908.028  1902.448  1907.478   1909.649   \n",
       "2021-01-06 13:00:00  1907.478  1911.348  1905.968  1911.248   1907.978   \n",
       "2021-01-06 14:00:00  1911.158  1921.208  1908.488  1918.365   1907.478   \n",
       "2021-01-06 15:00:00  1918.305  1924.175  1915.208  1918.088   1911.158   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2023-02-28 12:00:00  1826.518  1831.045  1826.255  1829.168   1818.608   \n",
       "2023-02-28 14:00:00  1827.978  1830.218  1827.848  1828.915   1826.518   \n",
       "2023-02-28 16:00:00  1826.375  1827.345  1825.316  1826.648   1827.978   \n",
       "2023-02-28 18:00:00  1826.104  1827.218  1825.265  1825.395   1826.375   \n",
       "2023-02-28 19:00:00  1825.374  1825.565  1822.938  1823.355   1826.104   \n",
       "\n",
       "                     open_lag2  open_lag3  open_lag4  open_lag5  high_lag1  \\\n",
       "datetime                                                                     \n",
       "2021-01-06 11:00:00   1932.825   1933.809   1945.168   1956.595   1933.658   \n",
       "2021-01-06 12:00:00   1932.345   1932.825   1933.809   1945.168   1909.848   \n",
       "2021-01-06 13:00:00   1909.649   1932.345   1932.825   1933.809   1908.028   \n",
       "2021-01-06 14:00:00   1907.978   1909.649   1932.345   1932.825   1911.348   \n",
       "2021-01-06 15:00:00   1907.478   1907.978   1909.649   1932.345   1921.208   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2023-02-28 12:00:00   1809.258   1808.915   1810.575   1810.425   1826.645   \n",
       "2023-02-28 14:00:00   1818.608   1809.258   1808.915   1810.575   1831.045   \n",
       "2023-02-28 16:00:00   1826.518   1818.608   1809.258   1808.915   1830.218   \n",
       "2023-02-28 18:00:00   1827.978   1826.518   1818.608   1809.258   1827.345   \n",
       "2023-02-28 19:00:00   1826.375   1827.978   1826.518   1818.608   1827.218   \n",
       "\n",
       "                     ...  low_lag3  low_lag4  low_lag5  close_lag1  \\\n",
       "datetime             ...                                             \n",
       "2021-01-06 11:00:00  ...  1930.588  1924.989  1945.168    1909.728   \n",
       "2021-01-06 12:00:00  ...  1927.288  1930.588  1924.989    1907.965   \n",
       "2021-01-06 13:00:00  ...  1903.895  1927.288  1930.588    1907.478   \n",
       "2021-01-06 14:00:00  ...  1900.755  1903.895  1927.288    1911.248   \n",
       "2021-01-06 15:00:00  ...  1902.448  1900.755  1903.895    1918.365   \n",
       "...                  ...       ...       ...       ...         ...   \n",
       "2023-02-28 12:00:00  ...  1807.745  1809.414  1804.594    1823.975   \n",
       "2023-02-28 14:00:00  ...  1807.108  1807.745  1809.414    1829.168   \n",
       "2023-02-28 16:00:00  ...  1818.608  1807.108  1807.745    1828.915   \n",
       "2023-02-28 18:00:00  ...  1826.255  1818.608  1807.108    1826.648   \n",
       "2023-02-28 19:00:00  ...  1827.848  1826.255  1818.608    1825.395   \n",
       "\n",
       "                     close_lag2  close_lag3  close_lag4  close_lag5  \\\n",
       "datetime                                                              \n",
       "2021-01-06 11:00:00    1932.395    1932.858    1933.849    1945.168   \n",
       "2021-01-06 12:00:00    1909.728    1932.395    1932.858    1933.849   \n",
       "2021-01-06 13:00:00    1907.965    1909.728    1932.395    1932.858   \n",
       "2021-01-06 14:00:00    1907.478    1907.965    1909.728    1932.395   \n",
       "2021-01-06 15:00:00    1911.248    1907.478    1907.965    1909.728   \n",
       "...                         ...         ...         ...         ...   \n",
       "2023-02-28 12:00:00    1813.804    1808.975    1810.235    1810.565   \n",
       "2023-02-28 14:00:00    1823.975    1813.804    1808.975    1810.235   \n",
       "2023-02-28 16:00:00    1829.168    1823.975    1813.804    1808.975   \n",
       "2023-02-28 18:00:00    1828.915    1829.168    1823.975    1813.804   \n",
       "2023-02-28 19:00:00    1826.648    1828.915    1829.168    1823.975   \n",
       "\n",
       "                            EMAF      STO_K  \n",
       "datetime                                     \n",
       "2021-01-06 11:00:00  1940.718746  15.858660  \n",
       "2021-01-06 12:00:00  1939.610721  13.895802  \n",
       "2021-01-06 13:00:00  1938.665297  19.664488  \n",
       "2021-01-06 14:00:00  1937.988620  32.465316  \n",
       "2021-01-06 15:00:00  1937.325266  51.887239  \n",
       "...                          ...        ...  \n",
       "2023-02-28 12:00:00  1818.791606  87.624677  \n",
       "2023-02-28 14:00:00  1819.129052  90.383977  \n",
       "2023-02-28 16:00:00  1819.379684  88.297058  \n",
       "2023-02-28 18:00:00  1819.580194  75.767876  \n",
       "2023-02-28 19:00:00  1819.706021  47.115230  \n",
       "\n",
       "[12633 rows x 26 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####เอาไว้เรียนรู้ทำความเข้าใจ ตรงนี้สำคัญ ยากและลืมง่าย\n",
    "\n",
    "#Create a sample dataframe\n",
    "df = pd.DataFrame({'value': [2, 4, 5, 7, 6, 8, 9, 10, 11, 12, 9, 8, 6, 4, 3]})\n",
    "\n",
    "#Apply rolling window with current row included and apply max function\n",
    "window_period = 4\n",
    "df['rolling_min'] = df['value'].rolling(window=4).min().shift(-3)\n",
    "df['rolling_max'] = df['value'].rolling(window=4).max().shift(-3)\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Manually Normaliz </summary>\n",
    "```python\n",
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894\n",
      " 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894 1894]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler_long.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(sum(X_test_n == X_test_scaled)) # sum if true\n",
    "\n",
    "dump(scaler, 'scaler_long.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test import scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894,\n",
       "       1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894,\n",
       "       1894, 1894, 1894, 1894])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "scaler2 = load('scaler_long.pkl')\n",
    "X_test_scaled2 = scaler2.transform(X_test)\n",
    "sum((X_test_n == X_test_scaled2) & (X_test_scaled == X_test_scaled2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean scaler one [1823.14996009 1825.42356581 1820.74145828 1823.12533786 1823.16886906\n",
      " 1823.18806479 1823.20759159 1823.22809181 1823.24962743 1825.44232203\n",
      " 1825.4615709  1825.48193725 1825.50277408 1825.52428471 1820.7582595\n",
      " 1820.77723847 1820.7964922  1820.81515513 1820.83565604 1823.14222886\n",
      " 1823.16112653 1823.18031739 1823.19984193 1823.22034102 1823.6406115\n",
      "   51.27082431]\n",
      "mean scaler two [1823.14996009 1825.42356581 1820.74145828 1823.12533786 1823.16886906\n",
      " 1823.18806479 1823.20759159 1823.22809181 1823.24962743 1825.44232203\n",
      " 1825.4615709  1825.48193725 1825.50277408 1825.52428471 1820.7582595\n",
      " 1820.77723847 1820.7964922  1820.81515513 1820.83565604 1823.14222886\n",
      " 1823.16112653 1823.18031739 1823.19984193 1823.22034102 1823.6406115\n",
      "   51.27082431]\n",
      "mean manual nor [1823.14996009 1825.42356581 1820.74145828 1823.12533786 1823.16886906\n",
      " 1823.18806479 1823.20759159 1823.22809181 1823.24962743 1825.44232203\n",
      " 1825.4615709  1825.48193725 1825.50277408 1825.52428471 1820.7582595\n",
      " 1820.77723847 1820.7964922  1820.81515513 1820.83565604 1823.14222886\n",
      " 1823.16112653 1823.18031739 1823.19984193 1823.22034102 1823.6406115\n",
      "   51.27082431]\n",
      "std scaler one [62.06802169 62.35242421 61.75748684 62.06409554 62.07581111 62.08346812\n",
      " 62.09109918 62.10143184 62.11465169 62.36002974 62.36851291 62.37763152\n",
      " 62.38792326 62.40072893 61.75989448 61.76683767 61.77452447 61.7810666\n",
      " 61.7922442  62.06733469 62.07515234 62.08282673 62.09047515 62.10081154\n",
      " 61.06694421 23.93620108]\n",
      "std scaler two [62.06802169 62.35242421 61.75748684 62.06409554 62.07581111 62.08346812\n",
      " 62.09109918 62.10143184 62.11465169 62.36002974 62.36851291 62.37763152\n",
      " 62.38792326 62.40072893 61.75989448 61.76683767 61.77452447 61.7810666\n",
      " 61.7922442  62.06733469 62.07515234 62.08282673 62.09047515 62.10081154\n",
      " 61.06694421 23.93620108]\n",
      "std manual nor [62.06802169 62.35242421 61.75748684 62.06409554 62.07581111 62.08346812\n",
      " 62.09109918 62.10143184 62.11465169 62.36002974 62.36851291 62.37763152\n",
      " 62.38792326 62.40072893 61.75989448 61.76683767 61.77452447 61.7810666\n",
      " 61.7922442  62.06733469 62.07515234 62.08282673 62.09047515 62.10081154\n",
      " 61.06694421 23.93620108]\n",
      "[ 1.39361683  1.353988    1.29560877  1.36696848  1.7587548   1.76596022\n",
      "  1.78127638  1.96356033  2.14676198  1.73533718  1.80529283  1.89132001\n",
      "  1.97060937  2.11942581  1.34612828  1.72440043  1.77729426  1.68617751\n",
      "  2.01210274  1.39502963  1.75970367  1.76663481  1.78206332  1.96370476\n",
      "  1.91720964 -1.47943962]\n"
     ]
    }
   ],
   "source": [
    "mean = scaler.mean_\n",
    "std = scaler.scale_\n",
    "mean2 = scaler2.mean_\n",
    "std2 = scaler2.scale_\n",
    "print('mean scaler one',mean)\n",
    "print('mean scaler two',mean2)\n",
    "print('mean manual nor',X_train_means)\n",
    "\n",
    "print('std scaler one',std)\n",
    "print('std scaler two',std2)\n",
    "print('std manual nor',X_train_stds)\n",
    "\n",
    "print(X_train_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), \"C\" is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "The hyperparameter \"C\" in SVM controls the misclassification penalty. A smaller value of \"C\" allows more misclassifications on the training data, while a larger value of \"C\" penalizes misclassifications more heavily. In other words, a smaller value of \"C\" creates a wider margin, allowing more data points to fall within the margin, but may result in lower accuracy on the training data. Conversely, a larger value of \"C\" creates a narrower margin, reducing the number of misclassifications but may lead to overfitting.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Large C >>> Larger penalizes >>> complex model >>> lead to overfittting <br>\n",
    "Small C >>> Smaller penalizes >>> simple model >>> underfitting\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052770448548813\n",
      "748\n",
      "1147\n"
     ]
    }
   ],
   "source": [
    "print((y_cross == 0).sum() /((y_cross == 1).sum() + (y_cross == 0).sum() ))\n",
    "print((y_cross == 1).sum())\n",
    "print((y_cross == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pkong\\LSVM-XAUUSD\\lsvm_Long.ipynb Cell 33\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Loop through the hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m C \u001b[39min\u001b[39;00m Cs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter, n_jobs=-1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     lsvm \u001b[39m=\u001b[39m SVC(kernel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, C\u001b[39m=\u001b[39;49mC, max_iter\u001b[39m=\u001b[39;49mmax_iter, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     lsvm\u001b[39m.\u001b[39mfit(features_X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pkong/LSVM-XAUUSD/lsvm_Long.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     y_train_pred \u001b[39m=\u001b[39m lsvm\u001b[39m.\u001b[39mpredict(features_X_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "# Define a list of hyperparameters to loop through\n",
    "#################larger value of C imposes less regularization on the model\n",
    "Cs = [0.001,0.01, 0.1,0.5, 1, 1.5, 2, 10,100]\n",
    "# Cs = [0.001,0.01,0.5,1,10]\n",
    "\n",
    "# Initialize variables to keep track of the best hyperparameters and their corresponding score\n",
    "best_score = 0\n",
    "best_C = None\n",
    "max_iter = 20000\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "# Loop through the hyperparameters\n",
    "for C in Cs:\n",
    "    lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter)\n",
    "    lsvm.fit(features_X_train, y_train)\n",
    "    y_train_pred = lsvm.predict(features_X_train)\n",
    "    tn, fp, fn, tp = confusion_matrix(target_y_train, y_train_pred).ravel()\n",
    "    train_acc = tp / (fp + tp)\n",
    "    train_score = lsvm.score(features_X_train, target_y_train)\n",
    "    y_cross_pred = lsvm.predict(features_X_cross)\n",
    "    tn, fp, fn, tp = confusion_matrix(target_y_cross, y_cross_pred).ravel()\n",
    "    cross_acc = tp / (fp + tp)\n",
    "    cross_score = lsvm.score(features_X_cross, target_y_cross)\n",
    "    print(f\"C={C}, Train acc={train_acc}, cross acc={cross_acc}\")\n",
    "    print(f\"C={C}, Train score={train_score}, cross score={cross_score}\")\n",
    "    \n",
    "    # Update the best hyperparameters and their corresponding score if applicable\n",
    "    if (train_acc + cross_acc) > best_score:\n",
    "        best_score = train_acc + cross_acc\n",
    "        best_C = C\n",
    "\n",
    "print(\"The best 'C' is:\", best_C)\n",
    "# Create the LinearSVC model with normalization and the best value of C\n",
    "lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=best_C, max_iter=max_iter)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lsvm.fit(features_X_train, target_y_train)\n",
    "\n",
    "\n",
    "#EVALUATION\n",
    "yhat_m1 = lsvm.predict(features_X_cross)\n",
    "accuracy_m1 = round(accuracy_score(target_y_cross,yhat_m1),2)\n",
    "print('Accuracy_score: ', accuracy_m1)\n",
    "f1 = round(f1_score(target_y_cross, yhat_m1, average='weighted'), 2)\n",
    "print('weight avg', f1)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross, yhat_m1))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target_y_cross, yhat_m1).ravel()\n",
    "tpr = (tp / (tp + fp)) * 100\n",
    "print(\"Accuracy for predicting class 1: {:.2f}%\".format(tpr))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(lsvm, features_X_cross, target_y_cross)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> XGBoost </summary>\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint # this will indirectly use in TimeSeriesSplit\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_dist = {'n_estimators': [100, 200, 300],\n",
    "              'max_depth': [3, 6, 9, 18, 36],\n",
    "              'gamma': [0.01, 0.1],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "              'reg_lambda': [0.1, 1, 10],\n",
    "              'reg_alpha': [0.1, 1, 10]}\n",
    "\n",
    "def count_hyperparam_combinations(param_dict):\n",
    "    total_combinations = 1\n",
    "    for key in param_dict:\n",
    "        total_combinations *= len(param_dict[key])\n",
    "    return total_combinations\n",
    "\n",
    "possible_combination = count_hyperparam_combinations(param_dist)\n",
    "print(\"possible combination:\", possible_combination)\n",
    "\n",
    "\n",
    "# Create a time series split object\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Scale the training data\n",
    "scaler = StandardScaler()\n",
    "features_X_train = scaler.fit_transform(X_train)\n",
    "target_y_train = y_train\n",
    "\n",
    "\n",
    "\n",
    "# Create a random search object\n",
    "model_xgb = xgb.XGBRegressor(seed=42) # set seed to 42 to fix how model randomize sets of cv, this is for reproducibility.\n",
    "random_search = RandomizedSearchCV(model_xgb, param_distributions=param_dist, cv=tscv, n_iter=possible_combination-(possible_combination*0.8), verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "random_search.fit(features_X_train, target_y_train)\n",
    "\n",
    "# Print the best hyperparameters found by the random search\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Create the final model using the best hyperparameters\n",
    "model_xgb = xgb.XGBRegressor(seed=42, **random_search.best_params_)\n",
    "\n",
    "\n",
    "# Fit the final model to the training data\n",
    "model_xgb.fit(features_X_train, y_train)\n",
    "\n",
    "\n",
    "##########Evaluation\n",
    "# Normalize the test data using the same scaler used for training data\n",
    "features_X_cross = scaler.transform(X_cross)\n",
    "target_y_cross = y_cross\n",
    "\n",
    "# Make predictions using the XGBoost model\n",
    "yhat_m2_pred = model_xgb.predict(features_X_cross)\n",
    "\n",
    "# Convert the predicted values to binary labels (0 or 1) using a threshold of 0.5\n",
    "yhat_m2 = (yhat_m2_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model performance on the test set\n",
    "accuracy = accuracy_score(target_y_cross, yhat_m2)\n",
    "f1 = f1_score(target_y_cross, yhat_m2, average='weighted')\n",
    "jaccard = jaccard_score(target_y_cross, yhat_m2)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy score:\", accuracy)\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"Jaccard score:\", jaccard)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions using the XGBoost model\n",
    "yhat_m2_pred = model_xgb.predict(features_X_cross)\n",
    "\n",
    "# Convert the predicted values to binary labels (0 or 1) using a threshold of 0.5\n",
    "yhat_m2 = (yhat_m2_pred > 0.5).astype(int)\n",
    "\n",
    "#report\n",
    "print(metrics.classification_report(target_y_cross, yhat_m2))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(target_y_cross, yhat_m2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(target_y_cross))\n",
    "disp.plot()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target_y_cross, yhat_m2).ravel()\n",
    "tpr = (tp / (tp + fp)) * 100\n",
    "print(\"Accuracy for predicting class 1: {:.2f}%\".format(tpr))\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mad man method loop through everything\n",
    "#   larger value of C imposes less regularization on the model\n",
    "params = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "best_score_no_norm = 0\n",
    "best_params = {}\n",
    "\n",
    "features_X_train = X_train\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross\n",
    "target_y_cross = y_cross\n",
    "\n",
    "for c in params['C']:\n",
    "    for p in params['penalty']:\n",
    "        lr.set_params(C=c, penalty=p)\n",
    "        lr.fit(features_X_train, target_y_train)\n",
    "        score_train = lr.score(features_X_train,target_y_train)\n",
    "        score_cross = lr.score(features_X_cross, target_y_cross)\n",
    "        print(\"score_train: {0}, score_cross: {1}, C: {2}, penalty: {3}\".format(score_train,score_cross,c,p))\n",
    "        if score_train > best_score_no_norm:\n",
    "            best_score_no_norm = score_train\n",
    "            best_params_no_norm = {'C': c, 'penalty': p}\n",
    "print(\"Without Normalization\")\n",
    "print(\"Best hyperparameters:\", best_params_no_norm)\n",
    "print(\"Cross set score:\", best_score_no_norm)\n",
    "\n",
    "#------------\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "best_score_norm = 0\n",
    "best_params_norm = {}\n",
    "\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "\n",
    "for c in params['C']:\n",
    "    for p in params['penalty']:\n",
    "        \n",
    "        lr.set_params(C=c, penalty=p)\n",
    "        lr.fit(features_X_train, target_y_train)\n",
    "        \n",
    "        score_train = lr.score(features_X_train,target_y_train)\n",
    "        score_cross = lr.score(features_X_cross, target_y_cross)\n",
    "        print(\"score_train: {0}, score_cross: {1}, C: {2}, penalty: {3}\".format(score_train,score_cross,c,p))\n",
    "        if score_train > best_score_norm:\n",
    "            best_score_norm = score_train\n",
    "            best_params_norm = {'C': c, 'penalty': p}\n",
    "print(\"With Normalization\")\n",
    "print(\"Best hyperparameters:\", best_params_norm)\n",
    "print(\"Cross set score:\", best_score_norm)\n",
    "\n",
    "if best_score_no_norm >= best_score_norm:\n",
    "    best_params = best_params_no_norm\n",
    "    features_X_train = X_train\n",
    "    target_y_train = y_train\n",
    "    features_X_cross = X_cross\n",
    "    target_y_cross = y_cross\n",
    "    isnorm = False\n",
    "elif best_score_norm >= best_score_no_norm:\n",
    "    best_params = best_params_norm\n",
    "    isnorm = True\n",
    "else:\n",
    "    raise ValueError('no condition met')\n",
    "\n",
    "print(\"BEST COMBINATION hyperparameters: {0}, With normalization: {1}\".format(best_params,isnorm))\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', C=best_params['C'],penalty=best_params['penalty'])##(** is for unpacking dictionary)\n",
    "lr.fit(features_X_train, target_y_train)\n",
    "\n",
    "#Evaluation\n",
    "cross_score = lr.score(features_X_cross, target_y_cross)\n",
    "print(\"cross set score:\", cross_score)\n",
    "\n",
    "yhat_m3 = lr.predict(features_X_cross)\n",
    "\n",
    "accuracy_m3 = accuracy_score(target_y_cross,yhat_m3)\n",
    "print('Accuracy: ', accuracy_m3)\n",
    "\n",
    "jaccard_m3 = jaccard_score(target_y_cross, yhat_m3,pos_label=1)\n",
    "print('Jaccard' , jaccard_m3)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross,yhat_m3))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(target_y_cross, yhat_m3)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(target_y_cross))\n",
    "disp.plot()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target_y_cross, yhat_m3).ravel()\n",
    "tpr = (tp / (tp + fp)) * 100\n",
    "print(\"Accuracy for predicting class 1: {:.2f}%\".format(tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simu(model, data_simu_x_n, data_simu_y, initial_capital):\n",
    "    capital = initial_capital\n",
    "    win_count = 0\n",
    "    lost_count = 0\n",
    "    no_order_count = 0\n",
    "    record_result = []\n",
    "    record_capital = []\n",
    "    record_date = []\n",
    "    consecutive_lost = 0\n",
    "    consecutive_lost_max = 0\n",
    "    lw = []\n",
    "    for i in range(0, len(data_simu_x_n)):\n",
    "        # if model == model_xgb:\n",
    "        #     prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        #     prediction = (prediction > 0.5).astype(int)\n",
    "        prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        \n",
    "        if prediction == 1 and data_simu_y[i] == 1:\n",
    "            outcome = 3.1\n",
    "            capital += outcome\n",
    "            win_count += 1\n",
    "            order_record = 'win-------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'win'\n",
    "        elif prediction == 1 and data_simu_y[i] == 0:\n",
    "            outcome = -3.1\n",
    "            capital += outcome\n",
    "            lost_count += 1\n",
    "            order_record = 'lost------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'lost'\n",
    "        elif prediction == 0:\n",
    "            no_order_count +=1\n",
    "            order_record = 'no order--- prediction = {0}, actual = {1}'.format(prediction, data_simu_y[i])\n",
    "            capital = capital\n",
    "            result = 'no order'\n",
    "        else:\n",
    "            raise ValueError('no condition met')\n",
    "        record_date.append(data_simu_x_n.iloc[i].name)\n",
    "        record_result.append(order_record)\n",
    "        record_capital.append(capital)\n",
    "\n",
    "        #Calculate Consecutive Lost\n",
    "        if result == 'lost':\n",
    "            consecutive_lost += -1\n",
    "            lw.append('Lost')\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "        if result == 'win':\n",
    "            lw.append('Win')\n",
    "            consecutive_lost = 0\n",
    "\n",
    "    total_return = ((capital - initial_capital) / initial_capital)*100\n",
    "    sim_df = pd.DataFrame({'record_date':record_date,\n",
    "                           'record_result':record_result,\n",
    "                           'record_capital': record_capital})\n",
    "    sim_df.set_index('record_date', inplace=True)\n",
    "\n",
    "    return win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max, lw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----cross-----\n",
      "The value should be:>> yhat = 0: 1814<<, >> yhat = 1: 81<<\n",
      "1814\n",
      "81\n",
      "-----test-----\n",
      "1793\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.asarray(X.loc[train_start:train_end])\n",
    "# y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "# X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "# y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "# X_test = np.asarray(X.loc[test_start:])\n",
    "# y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "X_test_simu = X.loc[test_start:]\n",
    "X_test_simu_n = normalize_data_new(X_test_simu,X_train_means,X_train_stds)\n",
    "\n",
    "X_cross_simu = X.loc[cross_start:cross_end]\n",
    "X_cross_simu_n = normalize_data_new(X_cross_simu,X_train_means,X_train_stds)\n",
    "\n",
    "def predict_count(data_simu):\n",
    "    adding = []\n",
    "    for i in range(0,len(data_simu)):\n",
    "        prediction = str(lsvm.predict(data_simu.iloc[i].values.reshape(1,-1))).strip('[]')\n",
    "        adding.append(prediction)\n",
    "    print(adding.count('0'))\n",
    "    print(adding.count('1'))\n",
    "\n",
    "\n",
    "print('-----cross-----')\n",
    "print('The value should be:>> yhat = 0: {0}<<, >> yhat = 1: {1}<<'.format(np.count_nonzero(yhat_m1 == 0),np.count_nonzero(yhat_m1 == 1)))\n",
    "predict_count(X_cross_simu_n)\n",
    "\n",
    "print('-----test-----')\n",
    "predict_count(X_test_simu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Return: 1.0%\n",
      "Final Capital: $503.0\n",
      "Win Count:  51\n",
      "Lost Count:  50\n",
      "No Order Count:  1793\n",
      "Max Consecutive Lost: 11.0 trades\n",
      "Total day on trading: 121 days\n",
      "Accuarcy 50.50%\n"
     ]
    }
   ],
   "source": [
    "win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max, lw = predict_simu(lsvm, \n",
    "                                                                                                          X_test_simu_n, \n",
    "                                                                                                          y_test, \n",
    "                                                                                                          500)\n",
    "print('Total Return: {0}%'.format(round(total_return,0)))\n",
    "print('Final Capital: ${0}'.format(round(capital,0)))\n",
    "print('Win Count: ',win_count)\n",
    "print('Lost Count: ',lost_count)\n",
    "print('No Order Count: ', no_order_count)\n",
    "print('Max Consecutive Lost: {0} trades'.format(consecutive_lost_max/-1))\n",
    "day_one = X_test_simu_n.iloc[0].name.to_pydatetime().date()\n",
    "day_final = X_test_simu_n.iloc[len(X_test_simu_n)-1].name.to_pydatetime().date()\n",
    "print('Total day on trading: {0} days'.format((day_final - day_one).days))\n",
    "Accuracy = win_count / (win_count+lost_count)\n",
    "print(f\"Accuarcy {Accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_simu_n.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.plot(y='record_capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df['record_capital'].to_csv('record_capital.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the simulation data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_long_simu.csv', y_test, delimiter=',')\n",
    "X_test_simu_n.to_csv('X_long_simu.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To import the file use:\n",
    "\n",
    "```python\n",
    "y_test = np.loadtxt('y_long_simu.csv', delimiter=',')\n",
    "X_test_simu_n = pd.read_csv('X_long_simu.csv',index_col='datetime', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print('version should be 4.0:',pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsvm,open('lsvm_xauusd_long_rrr.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('lsvm_xauusd_long_rrr.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = [-2.839808,-2.806494,-2.828501,-2.791640]\n",
    "model_input = np.array(model_input).reshape(1,-1)\n",
    "model_input\n",
    "pickled_model.predict(model_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question on SVM\n",
    "\n",
    "##### what is the different between svm.SVC(kernel='linear') and LinearSVC in sklearn?\n",
    "\n",
    "svm.SVC(kernel='linear') and LinearSVC are both linear SVM classifiers available in scikit-learn, but there are some differences between them.\n",
    "\n",
    "svm.SVC(kernel='linear') is based on the LIBSVM library, which is written in C++, while LinearSVC is based on the LIBLINEAR library, which is written in C.\n",
    "\n",
    "One of the main differences between these two is the handling of multi-class classification. svm.SVC(kernel='linear') uses the one-vs-one approach, which trains a binary classifier for every pair of classes, while LinearSVC uses the one-vs-the-rest approach, which trains a binary classifier for each class against all the other classes.\n",
    "\n",
    "Another difference is the way they handle the regularization parameter C. In svm.SVC(kernel='linear'), C controls the tradeoff between maximizing the margin and minimizing the classification error. In LinearSVC, C controls the strength of regularization, where smaller values of C lead to stronger regularization.\n",
    "\n",
    "Overall, LinearSVC is faster and more memory-efficient than svm.SVC(kernel='linear') and is recommended for large datasets with many features, while svm.SVC(kernel='linear') may be more suitable for smaller datasets with few features or for multi-class classification.\n",
    "\n",
    "##### which is created first\n",
    "\n",
    "SVC(kernel='linear') was created first and it is a more flexible implementation of linear support vector machines. It can handle non-linearly separable data by using the kernel trick to transform the input data into a higher-dimensional feature space where the data becomes linearly separable.\n",
    "\n",
    "LinearSVC, on the other hand, is a more efficient implementation of linear support vector machines that only supports linear kernels. It uses a different algorithm for training the model that is more scalable and faster than the algorithm used by SVC(kernel='linear').\n",
    "\n",
    "Both models are useful for solving binary classification problems, but LinearSVC is generally preferred when the dataset is large and the number of features is high, while SVC(kernel='linear') is preferred when the dataset is smaller or when non-linearly separable data is involved.\n",
    "\n",
    "##### Definition of large dataset and high number of features\n",
    "\n",
    "The definitions of \"large dataset\" and \"high number of features\" are somewhat relative and can vary depending on the context of the problem and the available computational resources. However, as a general rule of thumb, a \"large dataset\" can refer to a dataset with tens of thousands or more samples, while a \"small dataset\" can refer to a dataset with a few hundred or a few thousand samples.\n",
    "\n",
    "Similarly, a \"high number of features\" can also be relative, but it generally refers to datasets where the number of features is comparable to or larger than the number of samples. For example, in genomics and bioinformatics, it is common to work with datasets with tens of thousands or even hundreds of thousands of features.\n",
    "\n",
    "In practice, when working with a large dataset with a high number of features, LinearSVC can be more computationally efficient and can handle such datasets more effectively. On the other hand, when working with a smaller dataset with a lower number of features, SVC(kernel='linear') can perform just as well and may offer better accuracy in some cases.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question XGBoost\n",
    "\n",
    "Random Forest and XGBoost are both popular machine learning algorithms used for regression and classification tasks. However, there are several key differences between these algorithms, which are outlined below:\n",
    "\n",
    "**During the training process**, XGBoost uses gradient boosting to optimize a loss function. The loss function measures the difference between the predicted and actual values and is chosen based on the specific problem being solved. For example, the mean squared error (MSE) loss function can be used for regression problems, and the cross-entropy loss function can be used for classification problems.\n",
    "\n",
    "**Model architecture:** Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to make a final prediction. Each decision tree in the Random Forest is built independently and does not depend on the other trees. On the other hand, XGBoost is a boosted tree algorithm that builds decision trees sequentially, where each new tree corrects the errors of the previous trees.\n",
    "\n",
    "**Tree construction:** In Random Forest, each decision tree is built by randomly selecting a subset of the features and the samples in the training set. This helps to reduce overfitting and improves the performance of the model. In XGBoost, each tree is built by greedily selecting the best split that maximizes the information gain.\n",
    "\n",
    "**Handling of missing data:** Random Forest can handle missing data by imputing the missing values with the mean or median of the feature. XGBoost can handle missing data by splitting the samples into two groups: one group with the missing value and one group without the missing value.\n",
    "\n",
    "**Regularization:** Random Forest does not have any regularization parameters. XGBoost has several regularization parameters, including the learning rate, which controls the step size during the gradient descent, and the regularization term, which penalizes complex models and helps to prevent overfitting.\n",
    "\n",
    "**Performance:** Random Forest is generally faster to train than XGBoost, especially for large datasets. However, XGBoost often outperforms Random Forest in terms of predictive accuracy, especially for complex tasks with high-dimensional features.\n",
    "\n",
    "**In summary,** Random Forest and XGBoost are both powerful machine learning algorithms with different strengths and weaknesses. Random Forest is a simple and fast algorithm that can handle missing data, while XGBoost is a more complex algorithm that can handle complex tasks and has better predictive accuracy, but may require more tuning and training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

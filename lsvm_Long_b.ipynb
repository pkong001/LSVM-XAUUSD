{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize:  True\n",
      "logged in:  True\n",
      "\n",
      "\n",
      "2023-04-21 12:50:22.503026 | Login:  114123121 | Balance:  484.96 | Equity:  484.96\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt5\n",
    "from account_credentials import LOGIN,PASSWORD,SERVER\n",
    "from datetime import datetime\n",
    "\n",
    "is_initialized = mt5.initialize()\n",
    "print('initialize: ', is_initialized)\n",
    "\n",
    "is_logged_in = mt5.login(LOGIN, PASSWORD, SERVER)\n",
    "print('logged in: ', is_logged_in)\n",
    "print('\\n')\n",
    "account_info = mt5.account_info()\n",
    "print(datetime.now(),\n",
    "    '| Login: ', account_info.login,\n",
    "    '| Balance: ', account_info.balance,\n",
    "    '| Equity: ' , account_info.equity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-11 14:00:00</th>\n",
       "      <td>1739.949</td>\n",
       "      <td>1746.057</td>\n",
       "      <td>1739.742</td>\n",
       "      <td>1745.883</td>\n",
       "      <td>7484</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-11 15:00:00</th>\n",
       "      <td>1745.863</td>\n",
       "      <td>1752.092</td>\n",
       "      <td>1745.790</td>\n",
       "      <td>1751.641</td>\n",
       "      <td>4708</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-11 16:00:00</th>\n",
       "      <td>1751.661</td>\n",
       "      <td>1751.661</td>\n",
       "      <td>1747.651</td>\n",
       "      <td>1749.891</td>\n",
       "      <td>3426</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-11 17:00:00</th>\n",
       "      <td>1749.905</td>\n",
       "      <td>1754.309</td>\n",
       "      <td>1748.860</td>\n",
       "      <td>1752.754</td>\n",
       "      <td>6697</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-11 18:00:00</th>\n",
       "      <td>1752.769</td>\n",
       "      <td>1754.302</td>\n",
       "      <td>1751.152</td>\n",
       "      <td>1751.500</td>\n",
       "      <td>3319</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 01:00:00</th>\n",
       "      <td>2004.606</td>\n",
       "      <td>2005.204</td>\n",
       "      <td>2000.544</td>\n",
       "      <td>2002.051</td>\n",
       "      <td>4565</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 02:00:00</th>\n",
       "      <td>2002.062</td>\n",
       "      <td>2004.008</td>\n",
       "      <td>2001.500</td>\n",
       "      <td>2003.036</td>\n",
       "      <td>3181</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 03:00:00</th>\n",
       "      <td>2003.002</td>\n",
       "      <td>2003.265</td>\n",
       "      <td>2001.144</td>\n",
       "      <td>2002.096</td>\n",
       "      <td>2999</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 04:00:00</th>\n",
       "      <td>2002.129</td>\n",
       "      <td>2002.307</td>\n",
       "      <td>2000.351</td>\n",
       "      <td>2000.568</td>\n",
       "      <td>2403</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 05:00:00</th>\n",
       "      <td>2000.600</td>\n",
       "      <td>2001.048</td>\n",
       "      <td>1994.732</td>\n",
       "      <td>1995.221</td>\n",
       "      <td>3089</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close  volume  spread  \\\n",
       "date                                                                          \n",
       "2021-08-11 14:00:00  1739.949  1746.057  1739.742  1745.883    7484     125   \n",
       "2021-08-11 15:00:00  1745.863  1752.092  1745.790  1751.641    4708     125   \n",
       "2021-08-11 16:00:00  1751.661  1751.661  1747.651  1749.891    3426     125   \n",
       "2021-08-11 17:00:00  1749.905  1754.309  1748.860  1752.754    6697     125   \n",
       "2021-08-11 18:00:00  1752.769  1754.302  1751.152  1751.500    3319     125   \n",
       "...                       ...       ...       ...       ...     ...     ...   \n",
       "2023-04-21 01:00:00  2004.606  2005.204  2000.544  2002.051    4565     125   \n",
       "2023-04-21 02:00:00  2002.062  2004.008  2001.500  2003.036    3181     125   \n",
       "2023-04-21 03:00:00  2003.002  2003.265  2001.144  2002.096    2999     125   \n",
       "2023-04-21 04:00:00  2002.129  2002.307  2000.351  2000.568    2403     125   \n",
       "2023-04-21 05:00:00  2000.600  2001.048  1994.732  1995.221    3089     125   \n",
       "\n",
       "                     real_volume  \n",
       "date                              \n",
       "2021-08-11 14:00:00            0  \n",
       "2021-08-11 15:00:00            0  \n",
       "2021-08-11 16:00:00            0  \n",
       "2021-08-11 17:00:00            0  \n",
       "2021-08-11 18:00:00            0  \n",
       "...                          ...  \n",
       "2023-04-21 01:00:00            0  \n",
       "2023-04-21 02:00:00            0  \n",
       "2023-04-21 03:00:00            0  \n",
       "2023-04-21 04:00:00            0  \n",
       "2023-04-21 05:00:00            0  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = 'XAUUSD'\n",
    "number_of_date= 10000\n",
    "timeframe = mt5.TIMEFRAME_H1\n",
    "from_date = datetime.now()\n",
    "\n",
    "df = pd.DataFrame(mt5.copy_rates_from(symbol,timeframe,from_date,number_of_date))\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "df = df.rename(columns={'time': 'date','tick_volume':'volume'})\n",
    "df = df.set_index(\"date\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding shift windows\n",
    "# targets, check the highest gold price attained in the next 4 hours\n",
    "highs = df['high'].rolling(window=4).max().shift(-4)\n",
    "lows = df['low'].rolling(window=4).min().shift(-4)\n",
    "\n",
    "# create new columns for conditions\n",
    "df['high_close_diff'] = highs - df['close'].shift(1)\n",
    "df['low_close_diff'] = lows - df['close'].shift(1)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9995, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Condition for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3095\n",
      "0 6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkong\\AppData\\Local\\Temp\\ipykernel_18008\\2025488886.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df.apply(reco,axis=1)\n"
     ]
    }
   ],
   "source": [
    "def reco(row):\n",
    "    if row.low_close_diff <= -3.3:\n",
    "        return 0\n",
    "    elif row.high_close_diff >= 4.3: #I use 3 plus .3 for bit offer off-set, and 1 in case of delay ordering cause price to change.\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['target'] = df.apply(reco,axis=1)\n",
    "\n",
    "\n",
    "print('1', (df['target'] == 1).sum())\n",
    "print('0', (df['target'] == 0).sum())\n",
    "\n",
    "df_simu = df\n",
    "df = df[['open','high','low','close','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9995, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Training, Cross Validation, and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9995\n",
      "number of training set:  7996\n",
      "number of cross validation set:  1000\n",
      "number of test set:  1000\n",
      "sum 9996\n",
      "Training set start: 2021-08-11 15:00:00 \n",
      " Training set end: 2022-12-15 21:00:00\n",
      "cross validation set start: 2022-12-15 23:00:00 \n",
      " cross validation set end: 2023-02-17 11:00:00\n",
      "test set start: 2023-02-17 12:00:00 \n",
      " test set end: 2023-04-21 01:00:00\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(n)\n",
    "\n",
    "\n",
    "train_n = int(round(n*0.80,0))\n",
    "print('number of training set: ', train_n)\n",
    "\n",
    "cross_n = int(round(n*0.10,0))\n",
    "print('number of cross validation set: ', cross_n)\n",
    "\n",
    "test_n = int(round(n*0.10,0))\n",
    "print('number of test set: ', test_n)\n",
    "\n",
    "print('sum', train_n + cross_n + test_n)\n",
    "\n",
    "\n",
    "train_start = str(df.iloc[0].name)\n",
    "train_end = str(df.iloc[train_n].name)\n",
    "print('Training set start: {0} \\n Training set end: {1}'.format(train_start,train_end))\n",
    "#print(df.loc[train_start:train_end])\n",
    "\n",
    "cross_start = str(df.iloc[train_n+1].name)\n",
    "cross_end = str(df.iloc[train_n+cross_n].name)\n",
    "print('cross validation set start: {0} \\n cross validation set end: {1}'.format(cross_start,cross_end))\n",
    "#print(df.loc[cross_start:cross_end])\n",
    "\n",
    "test_start = str(df.iloc[train_n + cross_n +1].name)\n",
    "test_end = str(df.iloc[n-1].name)\n",
    "print('test set start: {0} \\n test set end: {1}'.format(test_start,test_end))\n",
    "#print(df.loc[test_start:])\n",
    "\n",
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train = np.asarray(X.loc[train_start:train_end])\n",
    "y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "X_test = np.asarray(X.loc[test_start:])\n",
    "y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "\n",
    "#Extra for GridSearch\n",
    "X_train_cv = np.asarray(X.loc[train_start:cross_end])\n",
    "y_train_cv = np.asarray(y.loc[train_start:cross_end])\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape, X_cross.shape, y_cross.shape, X_test.shape, y_test.shape, X_train_cv.shape, y_train_cv.shape\n",
    "\n",
    "\n",
    "# Below is for later simulation\n",
    "df_simu_test = df_simu.loc[test_start:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####เอาไว้เรียนรู้ทำความเข้าใจ ตรงนี้สำคัญ ยากและลืมง่าย\n",
    "\n",
    "#Create a sample dataframe\n",
    "df = pd.DataFrame({'value': [2, 4, 5, 7, 6, 8, 9, 10, 11, 12, 9, 8, 6, 4, 3]})\n",
    "\n",
    "#Apply rolling window with current row included and apply max function\n",
    "window_period = 4\n",
    "df['rolling_min'] = df['value'].rolling(window=4).min().shift(-3)\n",
    "df['rolling_max'] = df['value'].rolling(window=4).max().shift(-3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_long_b.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "sum(X_test_n != X_test_scaled)\n",
    "\n",
    "dump(scaler, 'scaler_long_b.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test import scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([998, 998, 998, 998])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "scaler2 = load('scaler_long_b.pkl')\n",
    "X_test_scaled2 = scaler2.transform(X_test)\n",
    "sum((X_test_n == X_test_scaled2) & (X_test_scaled == X_test_scaled2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean scaler one [1798.41130749 1800.63282081 1796.08949218 1798.40838564]\n",
      "mean scaler two [1798.41130749 1800.63282081 1796.08949218 1798.40838564]\n",
      "mean manual nor [1798.41130749 1800.63282081 1796.08949218 1798.40838564]\n",
      "std scaler one [80.42596151 80.64360464 80.12784688 80.41029256]\n",
      "std scaler two [80.42596151 80.64360464 80.12784688 80.41029256]\n",
      "std manual nor [80.42596151 80.64360464 80.12784688 80.41029256]\n"
     ]
    }
   ],
   "source": [
    "mean = scaler.mean_\n",
    "std = scaler.scale_\n",
    "mean2 = scaler2.mean_\n",
    "std2 = scaler2.scale_\n",
    "print('mean scaler one',mean)\n",
    "print('mean scaler two',mean2)\n",
    "print('mean manual nor',X_train_means)\n",
    "\n",
    "print('std scaler one',std)\n",
    "print('std scaler two',std2)\n",
    "print('std manual nor',X_train_stds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), \"C\" is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "The hyperparameter \"C\" in SVM controls the misclassification penalty. A smaller value of \"C\" allows more misclassifications on the training data, while a larger value of \"C\" penalizes misclassifications more heavily. In other words, a smaller value of \"C\" creates a wider margin, allowing more data points to fall within the margin, but may result in lower accuracy on the training data. Conversely, a larger value of \"C\" creates a narrower margin, reducing the number of misclassifications but may lead to overfitting.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Large C >>> Larger penalizes >>> complex model >>> lead to overfittting <br>\n",
    "Small C >>> Smaller penalizes >>> simple model >>> underfitting\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01, Train score=0.7005126922595973, cross score=0.652\n",
      "C=0.1, Train score=0.7275228210578967, cross score=0.672\n",
      "C=0.5, Train score=0.7549080905339502, cross score=0.715\n",
      "C=1, Train score=0.7570338877078905, cross score=0.725\n",
      "C=1.5, Train score=0.7600350131299237, cross score=0.729\n",
      "C=2, Train score=0.7596598724521696, cross score=0.731\n",
      "C=10, Train score=0.7619107165186945, cross score=0.735\n",
      "The best 'C' is: 10\n",
      "score: 0.74\n",
      "Accuracy_score:  0.74\n",
      "weight avg 0.7\n",
      "jaccard:  0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.82       652\n",
      "           1       0.78      0.33      0.47       348\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.75      0.64      0.65      1000\n",
      "weighted avg       0.75      0.73      0.70      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of hyperparameters to loop through\n",
    "#################larger value of C imposes less regularization on the model\n",
    "Cs = [0.01, 0.1,0.5, 1, 1.5, 2, 10]\n",
    "\n",
    "# Initialize variables to keep track of the best hyperparameters and their corresponding score\n",
    "best_score = 0\n",
    "best_C = None\n",
    "max_iter = 20000\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "# Loop through the hyperparameters\n",
    "for C in Cs:\n",
    "    lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter)\n",
    "    lsvm.fit(X_train_n, y_train)\n",
    "    train_score = lsvm.score(features_X_train, target_y_train)\n",
    "    cross_score = lsvm.score(features_X_cross, target_y_cross)\n",
    "    print(f\"C={C}, Train score={train_score}, cross score={cross_score}\")\n",
    "    \n",
    "    # Update the best hyperparameters and their corresponding score if applicable\n",
    "    if train_score > best_score:\n",
    "        best_score = train_score\n",
    "        best_C = C\n",
    "\n",
    "print(\"The best 'C' is:\", best_C)\n",
    "# Create the LinearSVC model with normalization and the best value of C\n",
    "lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=best_C, max_iter=max_iter)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lsvm.fit(features_X_train, target_y_train)\n",
    "\n",
    "\n",
    "#EVALUATION\n",
    "test_score = round(lsvm.score(features_X_cross, target_y_cross),2)\n",
    "print(\"score:\", test_score)\n",
    "\n",
    "\n",
    "yhat_m1 = lsvm.predict(features_X_cross)\n",
    "accuracy_m1 = round(accuracy_score(target_y_cross,yhat_m1),2)\n",
    "print('Accuracy_score: ', accuracy_m1)\n",
    "\n",
    "f1 = round(f1_score(target_y_cross, yhat_m1, average='weighted'), 2)\n",
    "print('weight avg', f1)\n",
    "\n",
    "jaccard = round(jaccard_score(target_y_cross, yhat_m1,pos_label=1),2)\n",
    "print('jaccard: ', jaccard)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross, yhat_m1))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(lsvm, features_X_cross, target_y_cross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----cross-----\n",
      "The value should be:>> yhat = 0: 850<<, >> yhat = 1: 150<<\n",
      "850\n",
      "150\n",
      "-----test-----\n",
      "843\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "X_test_simu = X.loc[test_start:]\n",
    "X_test_simu_n = normalize_data_new(X_test_simu,X_train_means,X_train_stds)\n",
    "\n",
    "X_cross_simu = X.loc[cross_start:cross_end]\n",
    "X_cross_simu_n = normalize_data_new(X_cross_simu,X_train_means,X_train_stds)\n",
    "\n",
    "def predict_count(data_simu):\n",
    "    adding = []\n",
    "    for i in range(0,len(data_simu)):\n",
    "        prediction = str(lsvm.predict(data_simu.iloc[i].values.reshape(1,-1))).strip('[]')\n",
    "        adding.append(prediction)\n",
    "    print(adding.count('0'))\n",
    "    print(adding.count('1'))\n",
    "\n",
    "\n",
    "print('-----cross-----')\n",
    "print('The value should be:>> yhat = 0: {0}<<, >> yhat = 1: {1}<<'.format(np.count_nonzero(yhat_m1 == 0),np.count_nonzero(yhat_m1 == 1)))\n",
    "predict_count(X_cross_simu_n)\n",
    "\n",
    "print('-----test-----')\n",
    "predict_count(X_test_simu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simu(model, data_simu_x_n, data_simu_y, initial_capital):\n",
    "    capital = initial_capital\n",
    "    win_count = 0\n",
    "    lost_count = 0\n",
    "    no_order_count = 0\n",
    "    record_result = []\n",
    "    record_capital = []\n",
    "    record_date = []\n",
    "    consecutive_lost = 0\n",
    "    consecutive_lost_max = 0\n",
    "    for i in range(0, len(data_simu_x_n)):\n",
    "        prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        \n",
    "        if prediction == 1 and data_simu_y[i] == 1:\n",
    "            outcome = 3\n",
    "            capital += outcome\n",
    "            win_count += 1\n",
    "            order_record = 'win-------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'win'\n",
    "        elif prediction == 1 and data_simu_y[i] == 0:\n",
    "            outcome = -3.3\n",
    "            capital += outcome\n",
    "            lost_count += 1\n",
    "            order_record = 'lost------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'lost'\n",
    "        elif prediction == 0:\n",
    "            no_order_count +=1\n",
    "            order_record = 'no order--- prediction = {0}, actual = {1}'.format(prediction, data_simu_y[i])\n",
    "            capital = capital\n",
    "            result = 'no order'\n",
    "        else:\n",
    "            raise ValueError('no condition met')\n",
    "        record_date.append(data_simu_x_n.iloc[i].name)\n",
    "        record_result.append(order_record)\n",
    "        record_capital.append(capital)\n",
    "\n",
    "        #Calculate Consecutive Lost\n",
    "        if result == 'lost':\n",
    "            consecutive_lost += -3.3\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "        if result == 'win':\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "            consecutive_lost = 0\n",
    "\n",
    "    total_return = ((capital - initial_capital) / initial_capital)*100\n",
    "    sim_df = pd.DataFrame({'record_date':record_date,\n",
    "                           'record_result':record_result,\n",
    "                           'record_capital': record_capital})\n",
    "    sim_df.set_index('record_date', inplace=True)\n",
    "\n",
    "    return win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Return: 43.0%\n",
      "Final Capital: $713.0\n",
      "Win Count:  115\n",
      "Lost Count:  40\n",
      "No Order Count:  843\n",
      "Max Consecutive Lost: 2.9999999999999996 trades\n",
      "Total day on trading: 62 days\n",
      "accuracy 0.74%\n"
     ]
    }
   ],
   "source": [
    "win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max = predict_simu(lsvm, \n",
    "                                                                                                          X_test_simu_n, \n",
    "                                                                                                          y_test, \n",
    "                                                                                                          500)\n",
    "print('Total Return: {0}%'.format(round(total_return,0)))\n",
    "print('Final Capital: ${0}'.format(round(capital,0)))\n",
    "print('Win Count: ',win_count)\n",
    "print('Lost Count: ',lost_count)\n",
    "print('No Order Count: ', no_order_count)\n",
    "print('Max Consecutive Lost: {0} trades'.format(consecutive_lost_max/-3.3))\n",
    "day_one = X_test_simu_n.iloc[0].name.to_pydatetime().date()\n",
    "day_final = X_test_simu_n.iloc[len(X_test_simu_n)-1].name.to_pydatetime().date()\n",
    "print('Total day on trading: {0} days'.format((day_final - day_one).days))\n",
    "accuracy = win_count/(win_count+lost_count)\n",
    "print(f\"accuracy {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.plot(y='record_capital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the simulation data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_long_simu_b.csv', y_test, delimiter=',')\n",
    "X_test_simu_n.to_csv('X_long_simu_b.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To import the file use:\n",
    "\n",
    "```python\n",
    "y_test = np.loadtxt('y_long_simu_b.csv', delimiter=',')\n",
    "X_test_simu_n = pd.read_csv('X_long_simu-b.csv',index_col='datetime', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version should be 4.0: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('version should be 4.0:',pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsvm,open('lsvm_xauusd_long_b.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('lsvm_xauusd_long_b.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = [-2.839808,-2.806494,-2.828501,-2.791640]\n",
    "model_input = np.array(model_input).reshape(1,-1)\n",
    "model_input\n",
    "pickled_model.predict(model_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question on SVM\n",
    "\n",
    "##### what is the different between svm.SVC(kernel='linear') and LinearSVC in sklearn?\n",
    "\n",
    "svm.SVC(kernel='linear') and LinearSVC are both linear SVM classifiers available in scikit-learn, but there are some differences between them.\n",
    "\n",
    "svm.SVC(kernel='linear') is based on the LIBSVM library, which is written in C++, while LinearSVC is based on the LIBLINEAR library, which is written in C.\n",
    "\n",
    "One of the main differences between these two is the handling of multi-class classification. svm.SVC(kernel='linear') uses the one-vs-one approach, which trains a binary classifier for every pair of classes, while LinearSVC uses the one-vs-the-rest approach, which trains a binary classifier for each class against all the other classes.\n",
    "\n",
    "Another difference is the way they handle the regularization parameter C. In svm.SVC(kernel='linear'), C controls the tradeoff between maximizing the margin and minimizing the classification error. In LinearSVC, C controls the strength of regularization, where smaller values of C lead to stronger regularization.\n",
    "\n",
    "Overall, LinearSVC is faster and more memory-efficient than svm.SVC(kernel='linear') and is recommended for large datasets with many features, while svm.SVC(kernel='linear') may be more suitable for smaller datasets with few features or for multi-class classification.\n",
    "\n",
    "##### which is created first\n",
    "\n",
    "SVC(kernel='linear') was created first and it is a more flexible implementation of linear support vector machines. It can handle non-linearly separable data by using the kernel trick to transform the input data into a higher-dimensional feature space where the data becomes linearly separable.\n",
    "\n",
    "LinearSVC, on the other hand, is a more efficient implementation of linear support vector machines that only supports linear kernels. It uses a different algorithm for training the model that is more scalable and faster than the algorithm used by SVC(kernel='linear').\n",
    "\n",
    "Both models are useful for solving binary classification problems, but LinearSVC is generally preferred when the dataset is large and the number of features is high, while SVC(kernel='linear') is preferred when the dataset is smaller or when non-linearly separable data is involved.\n",
    "\n",
    "##### Definition of large dataset and high number of features\n",
    "\n",
    "The definitions of \"large dataset\" and \"high number of features\" are somewhat relative and can vary depending on the context of the problem and the available computational resources. However, as a general rule of thumb, a \"large dataset\" can refer to a dataset with tens of thousands or more samples, while a \"small dataset\" can refer to a dataset with a few hundred or a few thousand samples.\n",
    "\n",
    "Similarly, a \"high number of features\" can also be relative, but it generally refers to datasets where the number of features is comparable to or larger than the number of samples. For example, in genomics and bioinformatics, it is common to work with datasets with tens of thousands or even hundreds of thousands of features.\n",
    "\n",
    "In practice, when working with a large dataset with a high number of features, LinearSVC can be more computationally efficient and can handle such datasets more effectively. On the other hand, when working with a smaller dataset with a lower number of features, SVC(kernel='linear') can perform just as well and may offer better accuracy in some cases.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question XGBoost\n",
    "\n",
    "Random Forest and XGBoost are both popular machine learning algorithms used for regression and classification tasks. However, there are several key differences between these algorithms, which are outlined below:\n",
    "\n",
    "**During the training process**, XGBoost uses gradient boosting to optimize a loss function. The loss function measures the difference between the predicted and actual values and is chosen based on the specific problem being solved. For example, the mean squared error (MSE) loss function can be used for regression problems, and the cross-entropy loss function can be used for classification problems.\n",
    "\n",
    "**Model architecture:** Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to make a final prediction. Each decision tree in the Random Forest is built independently and does not depend on the other trees. On the other hand, XGBoost is a boosted tree algorithm that builds decision trees sequentially, where each new tree corrects the errors of the previous trees.\n",
    "\n",
    "**Tree construction:** In Random Forest, each decision tree is built by randomly selecting a subset of the features and the samples in the training set. This helps to reduce overfitting and improves the performance of the model. In XGBoost, each tree is built by greedily selecting the best split that maximizes the information gain.\n",
    "\n",
    "**Handling of missing data:** Random Forest can handle missing data by imputing the missing values with the mean or median of the feature. XGBoost can handle missing data by splitting the samples into two groups: one group with the missing value and one group without the missing value.\n",
    "\n",
    "**Regularization:** Random Forest does not have any regularization parameters. XGBoost has several regularization parameters, including the learning rate, which controls the step size during the gradient descent, and the regularization term, which penalizes complex models and helps to prevent overfitting.\n",
    "\n",
    "**Performance:** Random Forest is generally faster to train than XGBoost, especially for large datasets. However, XGBoost often outperforms Random Forest in terms of predictive accuracy, especially for complex tasks with high-dimensional features.\n",
    "\n",
    "**In summary,** Random Forest and XGBoost are both powerful machine learning algorithms with different strengths and weaknesses. Random Forest is a simple and fast algorithm that can handle missing data, while XGBoost is a more complex algorithm that can handle complex tasks and has better predictive accuracy, but may require more tuning and training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

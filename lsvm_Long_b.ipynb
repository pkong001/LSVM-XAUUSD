{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize:  True\n",
      "logged in:  True\n",
      "\n",
      "\n",
      "2023-04-16 22:31:30.690913 | Login:  114123121 | Balance:  503.22 | Equity:  503.22\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt5\n",
    "from account_credentials import LOGIN,PASSWORD,SERVER\n",
    "from datetime import datetime\n",
    "\n",
    "is_initialized = mt5.initialize()\n",
    "print('initialize: ', is_initialized)\n",
    "\n",
    "is_logged_in = mt5.login(LOGIN, PASSWORD, SERVER)\n",
    "print('logged in: ', is_logged_in)\n",
    "print('\\n')\n",
    "account_info = mt5.account_info()\n",
    "print(datetime.now(),\n",
    "    '| Login: ', account_info.login,\n",
    "    '| Balance: ', account_info.balance,\n",
    "    '| Equity: ' , account_info.equity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-05 07:00:00</th>\n",
       "      <td>1809.790</td>\n",
       "      <td>1814.607</td>\n",
       "      <td>1809.385</td>\n",
       "      <td>1813.711</td>\n",
       "      <td>3257</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-05 08:00:00</th>\n",
       "      <td>1813.666</td>\n",
       "      <td>1814.092</td>\n",
       "      <td>1808.314</td>\n",
       "      <td>1810.968</td>\n",
       "      <td>2947</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-05 09:00:00</th>\n",
       "      <td>1810.923</td>\n",
       "      <td>1811.678</td>\n",
       "      <td>1809.663</td>\n",
       "      <td>1810.487</td>\n",
       "      <td>2297</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-05 10:00:00</th>\n",
       "      <td>1810.441</td>\n",
       "      <td>1813.413</td>\n",
       "      <td>1809.887</td>\n",
       "      <td>1813.400</td>\n",
       "      <td>2646</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-05 11:00:00</th>\n",
       "      <td>1813.445</td>\n",
       "      <td>1814.822</td>\n",
       "      <td>1811.938</td>\n",
       "      <td>1813.124</td>\n",
       "      <td>3172</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 16:00:00</th>\n",
       "      <td>1995.660</td>\n",
       "      <td>1999.656</td>\n",
       "      <td>1992.401</td>\n",
       "      <td>1999.557</td>\n",
       "      <td>6472</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 17:00:00</th>\n",
       "      <td>1999.569</td>\n",
       "      <td>2004.139</td>\n",
       "      <td>1998.596</td>\n",
       "      <td>2003.898</td>\n",
       "      <td>6228</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 18:00:00</th>\n",
       "      <td>2003.929</td>\n",
       "      <td>2006.847</td>\n",
       "      <td>2003.371</td>\n",
       "      <td>2006.345</td>\n",
       "      <td>3857</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 19:00:00</th>\n",
       "      <td>2006.376</td>\n",
       "      <td>2007.991</td>\n",
       "      <td>2004.981</td>\n",
       "      <td>2005.317</td>\n",
       "      <td>4350</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 20:00:00</th>\n",
       "      <td>2005.301</td>\n",
       "      <td>2005.652</td>\n",
       "      <td>2003.873</td>\n",
       "      <td>2004.290</td>\n",
       "      <td>2018</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close  volume  spread  \\\n",
       "date                                                                          \n",
       "2021-08-05 07:00:00  1809.790  1814.607  1809.385  1813.711    3257     187   \n",
       "2021-08-05 08:00:00  1813.666  1814.092  1808.314  1810.968    2947     187   \n",
       "2021-08-05 09:00:00  1810.923  1811.678  1809.663  1810.487    2297     187   \n",
       "2021-08-05 10:00:00  1810.441  1813.413  1809.887  1813.400    2646     187   \n",
       "2021-08-05 11:00:00  1813.445  1814.822  1811.938  1813.124    3172     187   \n",
       "...                       ...       ...       ...       ...     ...     ...   \n",
       "2023-04-14 16:00:00  1995.660  1999.656  1992.401  1999.557    6472     125   \n",
       "2023-04-14 17:00:00  1999.569  2004.139  1998.596  2003.898    6228     125   \n",
       "2023-04-14 18:00:00  2003.929  2006.847  2003.371  2006.345    3857     125   \n",
       "2023-04-14 19:00:00  2006.376  2007.991  2004.981  2005.317    4350     125   \n",
       "2023-04-14 20:00:00  2005.301  2005.652  2003.873  2004.290    2018     125   \n",
       "\n",
       "                     real_volume  \n",
       "date                              \n",
       "2021-08-05 07:00:00            0  \n",
       "2021-08-05 08:00:00            0  \n",
       "2021-08-05 09:00:00            0  \n",
       "2021-08-05 10:00:00            0  \n",
       "2021-08-05 11:00:00            0  \n",
       "...                          ...  \n",
       "2023-04-14 16:00:00            0  \n",
       "2023-04-14 17:00:00            0  \n",
       "2023-04-14 18:00:00            0  \n",
       "2023-04-14 19:00:00            0  \n",
       "2023-04-14 20:00:00            0  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = 'XAUUSD'\n",
    "number_of_date= 10000\n",
    "timeframe = mt5.TIMEFRAME_H1\n",
    "from_date = datetime.now()\n",
    "\n",
    "df = pd.DataFrame(mt5.copy_rates_from(symbol,timeframe,from_date,number_of_date))\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "df = df.rename(columns={'time': 'date','tick_volume':'volume'})\n",
    "df = df.set_index(\"date\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding shift windows\n",
    "# targets, check the highest gold price attained in the next 4 hours\n",
    "highs = df['high'].rolling(window=4).max().shift(-4)\n",
    "lows = df['low'].rolling(window=4).min().shift(-4)\n",
    "\n",
    "# create new columns for conditions\n",
    "df['high_close_diff'] = highs - df['close'].shift(1)\n",
    "df['low_close_diff'] = lows - df['close'].shift(1)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9995, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Condition for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3091\n",
      "0 6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkong\\AppData\\Local\\Temp\\ipykernel_15496\\2025488886.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df.apply(reco,axis=1)\n"
     ]
    }
   ],
   "source": [
    "def reco(row):\n",
    "    if row.low_close_diff <= -3.3:\n",
    "        return 0\n",
    "    elif row.high_close_diff >= 4.3: #I use 3 plus .3 for bit offer off-set, and 1 in case of delay ordering cause price to change.\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['target'] = df.apply(reco,axis=1)\n",
    "\n",
    "\n",
    "print('1', (df['target'] == 1).sum())\n",
    "print('0', (df['target'] == 0).sum())\n",
    "\n",
    "df_simu = df\n",
    "df = df[['open','high','low','close','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9995, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Training, Cross Validation, and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9995\n",
      "number of training set:  7996\n",
      "number of cross validation set:  1000\n",
      "number of test set:  1000\n",
      "sum 9996\n",
      "Training set start: 2021-08-05 08:00:00 \n",
      " Training set end: 2022-12-09 13:00:00\n",
      "cross validation set start: 2022-12-09 14:00:00 \n",
      " cross validation set end: 2023-02-13 03:00:00\n",
      "test set start: 2023-02-13 04:00:00 \n",
      " test set end: 2023-04-14 16:00:00\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(n)\n",
    "\n",
    "\n",
    "train_n = int(round(n*0.80,0))\n",
    "print('number of training set: ', train_n)\n",
    "\n",
    "cross_n = int(round(n*0.10,0))\n",
    "print('number of cross validation set: ', cross_n)\n",
    "\n",
    "test_n = int(round(n*0.10,0))\n",
    "print('number of test set: ', test_n)\n",
    "\n",
    "print('sum', train_n + cross_n + test_n)\n",
    "\n",
    "\n",
    "train_start = str(df.iloc[0].name)\n",
    "train_end = str(df.iloc[train_n].name)\n",
    "print('Training set start: {0} \\n Training set end: {1}'.format(train_start,train_end))\n",
    "#print(df.loc[train_start:train_end])\n",
    "\n",
    "cross_start = str(df.iloc[train_n+1].name)\n",
    "cross_end = str(df.iloc[train_n+cross_n].name)\n",
    "print('cross validation set start: {0} \\n cross validation set end: {1}'.format(cross_start,cross_end))\n",
    "#print(df.loc[cross_start:cross_end])\n",
    "\n",
    "test_start = str(df.iloc[train_n + cross_n +1].name)\n",
    "test_end = str(df.iloc[n-1].name)\n",
    "print('test set start: {0} \\n test set end: {1}'.format(test_start,test_end))\n",
    "#print(df.loc[test_start:])\n",
    "\n",
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train = np.asarray(X.loc[train_start:train_end])\n",
    "y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "X_test = np.asarray(X.loc[test_start:])\n",
    "y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "\n",
    "#Extra for GridSearch\n",
    "X_train_cv = np.asarray(X.loc[train_start:cross_end])\n",
    "y_train_cv = np.asarray(y.loc[train_start:cross_end])\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape, X_cross.shape, y_cross.shape, X_test.shape, y_test.shape, X_train_cv.shape, y_train_cv.shape\n",
    "\n",
    "\n",
    "# Below is for later simulation\n",
    "df_simu_test = df_simu.loc[test_start:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####เอาไว้เรียนรู้ทำความเข้าใจ ตรงนี้สำคัญ ยากและลืมง่าย\n",
    "\n",
    "#Create a sample dataframe\n",
    "df = pd.DataFrame({'value': [2, 4, 5, 7, 6, 8, 9, 10, 11, 12, 9, 8, 6, 4, 3]})\n",
    "\n",
    "#Apply rolling window with current row included and apply max function\n",
    "window_period = 4\n",
    "df['rolling_min'] = df['value'].rolling(window=4).min().shift(-3)\n",
    "df['rolling_max'] = df['value'].rolling(window=4).max().shift(-3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_long_b.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "sum(X_test_n != X_test_scaled)\n",
    "\n",
    "dump(scaler, 'scaler_long_b.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test import scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([998, 998, 998, 998])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "scaler2 = load('scaler_long_b.pkl')\n",
    "X_test_scaled2 = scaler2.transform(X_test)\n",
    "sum((X_test_n == X_test_scaled2) & (X_test_scaled == X_test_scaled2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean scaler one [1797.92501751 1800.15153182 1795.58296886 1797.91050857]\n",
      "mean scaler two [1797.92501751 1800.15153182 1795.58296886 1797.91050857]\n",
      "mean manual nor [1797.92501751 1800.15153182 1795.58296886 1797.91050857]\n",
      "std scaler one [80.63019312 80.84035098 80.35121326 80.62034018]\n",
      "std scaler two [80.63019312 80.84035098 80.35121326 80.62034018]\n",
      "std manual nor [80.63019312 80.84035098 80.35121326 80.62034018]\n"
     ]
    }
   ],
   "source": [
    "mean = scaler.mean_\n",
    "std = scaler.scale_\n",
    "mean2 = scaler2.mean_\n",
    "std2 = scaler2.scale_\n",
    "print('mean scaler one',mean)\n",
    "print('mean scaler two',mean2)\n",
    "print('mean manual nor',X_train_means)\n",
    "\n",
    "print('std scaler one',std)\n",
    "print('std scaler two',std2)\n",
    "print('std manual nor',X_train_stds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), \"C\" is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "The hyperparameter \"C\" in SVM controls the misclassification penalty. A smaller value of \"C\" allows more misclassifications on the training data, while a larger value of \"C\" penalizes misclassifications more heavily. In other words, a smaller value of \"C\" creates a wider margin, allowing more data points to fall within the margin, but may result in lower accuracy on the training data. Conversely, a larger value of \"C\" creates a narrower margin, reducing the number of misclassifications but may lead to overfitting.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Large C >>> Larger penalizes >>> complex model >>> lead to overfittting <br>\n",
    "Small C >>> Smaller penalizes >>> simple model >>> underfitting\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01, Train score=0.6997624109040891, cross score=0.657\n",
      "C=0.1, Train score=0.727397774165312, cross score=0.677\n",
      "C=0.5, Train score=0.7547830436413655, cross score=0.72\n",
      "C=1, Train score=0.756783793922721, cross score=0.731\n",
      "C=1.5, Train score=0.7594097786670001, cross score=0.735\n",
      "C=2, Train score=0.758159309741153, cross score=0.738\n",
      "C=10, Train score=0.7609103413780167, cross score=0.743\n",
      "The best 'C' is: 10\n",
      "score: 0.74\n",
      "Accuracy_score:  0.74\n",
      "weight avg 0.71\n",
      "jaccard:  0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.83       657\n",
      "           1       0.79      0.34      0.47       343\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.76      0.65      0.65      1000\n",
      "weighted avg       0.75      0.74      0.71      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of hyperparameters to loop through\n",
    "#################larger value of C imposes less regularization on the model\n",
    "Cs = [0.01, 0.1,0.5, 1, 1.5, 2, 10]\n",
    "\n",
    "# Initialize variables to keep track of the best hyperparameters and their corresponding score\n",
    "best_score = 0\n",
    "best_C = None\n",
    "max_iter = 20000\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "# Loop through the hyperparameters\n",
    "for C in Cs:\n",
    "    lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter)\n",
    "    lsvm.fit(X_train_n, y_train)\n",
    "    train_score = lsvm.score(features_X_train, target_y_train)\n",
    "    cross_score = lsvm.score(features_X_cross, target_y_cross)\n",
    "    print(f\"C={C}, Train score={train_score}, cross score={cross_score}\")\n",
    "    \n",
    "    # Update the best hyperparameters and their corresponding score if applicable\n",
    "    if train_score > best_score:\n",
    "        best_score = train_score\n",
    "        best_C = C\n",
    "\n",
    "print(\"The best 'C' is:\", best_C)\n",
    "# Create the LinearSVC model with normalization and the best value of C\n",
    "lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=best_C, max_iter=max_iter)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lsvm.fit(features_X_train, target_y_train)\n",
    "\n",
    "\n",
    "#EVALUATION\n",
    "test_score = round(lsvm.score(features_X_cross, target_y_cross),2)\n",
    "print(\"score:\", test_score)\n",
    "\n",
    "\n",
    "yhat_m1 = lsvm.predict(features_X_cross)\n",
    "accuracy_m1 = round(accuracy_score(target_y_cross,yhat_m1),2)\n",
    "print('Accuracy_score: ', accuracy_m1)\n",
    "\n",
    "f1 = round(f1_score(target_y_cross, yhat_m1, average='weighted'), 2)\n",
    "print('weight avg', f1)\n",
    "\n",
    "jaccard = round(jaccard_score(target_y_cross, yhat_m1,pos_label=1),2)\n",
    "print('jaccard: ', jaccard)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross, yhat_m1))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(lsvm, features_X_cross, target_y_cross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----cross-----\n",
      "The value should be:>> yhat = 0: 854<<, >> yhat = 1: 146<<\n",
      "854\n",
      "146\n",
      "-----test-----\n",
      "843\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "X_test_simu = X.loc[test_start:]\n",
    "X_test_simu_n = normalize_data_new(X_test_simu,X_train_means,X_train_stds)\n",
    "\n",
    "X_cross_simu = X.loc[cross_start:cross_end]\n",
    "X_cross_simu_n = normalize_data_new(X_cross_simu,X_train_means,X_train_stds)\n",
    "\n",
    "def predict_count(data_simu):\n",
    "    adding = []\n",
    "    for i in range(0,len(data_simu)):\n",
    "        prediction = str(lsvm.predict(data_simu.iloc[i].values.reshape(1,-1))).strip('[]')\n",
    "        adding.append(prediction)\n",
    "    print(adding.count('0'))\n",
    "    print(adding.count('1'))\n",
    "\n",
    "\n",
    "print('-----cross-----')\n",
    "print('The value should be:>> yhat = 0: {0}<<, >> yhat = 1: {1}<<'.format(np.count_nonzero(yhat_m1 == 0),np.count_nonzero(yhat_m1 == 1)))\n",
    "predict_count(X_cross_simu_n)\n",
    "\n",
    "print('-----test-----')\n",
    "predict_count(X_test_simu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simu(model, data_simu_x_n, data_simu_y, initial_capital):\n",
    "    capital = initial_capital\n",
    "    win_count = 0\n",
    "    lost_count = 0\n",
    "    no_order_count = 0\n",
    "    record_result = []\n",
    "    record_capital = []\n",
    "    record_date = []\n",
    "    consecutive_lost = 0\n",
    "    consecutive_lost_max = 0\n",
    "    for i in range(0, len(data_simu_x_n)):\n",
    "        prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        \n",
    "        if prediction == 1 and data_simu_y[i] == 1:\n",
    "            outcome = 3\n",
    "            capital += outcome\n",
    "            win_count += 1\n",
    "            order_record = 'win-------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'win'\n",
    "        elif prediction == 1 and data_simu_y[i] == 0:\n",
    "            outcome = -3.3\n",
    "            capital += outcome\n",
    "            lost_count += 1\n",
    "            order_record = 'lost------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'lost'\n",
    "        elif prediction == 0:\n",
    "            no_order_count +=1\n",
    "            order_record = 'no order--- prediction = {0}, actual = {1}'.format(prediction, data_simu_y[i])\n",
    "            capital = capital\n",
    "            result = 'no order'\n",
    "        else:\n",
    "            raise ValueError('no condition met')\n",
    "        record_date.append(data_simu_x_n.iloc[i].name)\n",
    "        record_result.append(order_record)\n",
    "        record_capital.append(capital)\n",
    "\n",
    "        #Calculate Consecutive Lost\n",
    "        if result == 'lost':\n",
    "            consecutive_lost += -3.3\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "        if result == 'win':\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "            consecutive_lost = 0\n",
    "\n",
    "    total_return = ((capital - initial_capital) / initial_capital)*100\n",
    "    sim_df = pd.DataFrame({'record_date':record_date,\n",
    "                           'record_result':record_result,\n",
    "                           'record_capital': record_capital})\n",
    "    sim_df.set_index('record_date', inplace=True)\n",
    "\n",
    "    return win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Return: 39.0%\n",
      "Final Capital: $694.0\n",
      "Win Count:  112\n",
      "Lost Count:  43\n",
      "No Order Count:  843\n",
      "Max Consecutive Lost: 2.9999999999999996 trades\n",
      "Total day on trading: 60 days\n",
      "accuracy 0.72%\n"
     ]
    }
   ],
   "source": [
    "win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max = predict_simu(lsvm, \n",
    "                                                                                                          X_test_simu_n, \n",
    "                                                                                                          y_test, \n",
    "                                                                                                          500)\n",
    "print('Total Return: {0}%'.format(round(total_return,0)))\n",
    "print('Final Capital: ${0}'.format(round(capital,0)))\n",
    "print('Win Count: ',win_count)\n",
    "print('Lost Count: ',lost_count)\n",
    "print('No Order Count: ', no_order_count)\n",
    "print('Max Consecutive Lost: {0} trades'.format(consecutive_lost_max/-3.3))\n",
    "day_one = X_test_simu_n.iloc[0].name.to_pydatetime().date()\n",
    "day_final = X_test_simu_n.iloc[len(X_test_simu_n)-1].name.to_pydatetime().date()\n",
    "print('Total day on trading: {0} days'.format((day_final - day_one).days))\n",
    "accuracy = win_count/(win_count+lost_count)\n",
    "print(f\"accuracy {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.plot(y='record_capital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the simulation data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_long_simu_b.csv', y_test, delimiter=',')\n",
    "X_test_simu_n.to_csv('X_long_simu_b.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To import the file use:\n",
    "\n",
    "```python\n",
    "y_test = np.loadtxt('y_long_simu_b.csv', delimiter=',')\n",
    "X_test_simu_n = pd.read_csv('X_long_simu-b.csv',index_col='datetime', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version should be 4.0: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('version should be 4.0:',pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsvm,open('lsvm_xauusd_long_b.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('lsvm_xauusd_long_b.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = [-2.839808,-2.806494,-2.828501,-2.791640]\n",
    "model_input = np.array(model_input).reshape(1,-1)\n",
    "model_input\n",
    "pickled_model.predict(model_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question on SVM\n",
    "\n",
    "##### what is the different between svm.SVC(kernel='linear') and LinearSVC in sklearn?\n",
    "\n",
    "svm.SVC(kernel='linear') and LinearSVC are both linear SVM classifiers available in scikit-learn, but there are some differences between them.\n",
    "\n",
    "svm.SVC(kernel='linear') is based on the LIBSVM library, which is written in C++, while LinearSVC is based on the LIBLINEAR library, which is written in C.\n",
    "\n",
    "One of the main differences between these two is the handling of multi-class classification. svm.SVC(kernel='linear') uses the one-vs-one approach, which trains a binary classifier for every pair of classes, while LinearSVC uses the one-vs-the-rest approach, which trains a binary classifier for each class against all the other classes.\n",
    "\n",
    "Another difference is the way they handle the regularization parameter C. In svm.SVC(kernel='linear'), C controls the tradeoff between maximizing the margin and minimizing the classification error. In LinearSVC, C controls the strength of regularization, where smaller values of C lead to stronger regularization.\n",
    "\n",
    "Overall, LinearSVC is faster and more memory-efficient than svm.SVC(kernel='linear') and is recommended for large datasets with many features, while svm.SVC(kernel='linear') may be more suitable for smaller datasets with few features or for multi-class classification.\n",
    "\n",
    "##### which is created first\n",
    "\n",
    "SVC(kernel='linear') was created first and it is a more flexible implementation of linear support vector machines. It can handle non-linearly separable data by using the kernel trick to transform the input data into a higher-dimensional feature space where the data becomes linearly separable.\n",
    "\n",
    "LinearSVC, on the other hand, is a more efficient implementation of linear support vector machines that only supports linear kernels. It uses a different algorithm for training the model that is more scalable and faster than the algorithm used by SVC(kernel='linear').\n",
    "\n",
    "Both models are useful for solving binary classification problems, but LinearSVC is generally preferred when the dataset is large and the number of features is high, while SVC(kernel='linear') is preferred when the dataset is smaller or when non-linearly separable data is involved.\n",
    "\n",
    "##### Definition of large dataset and high number of features\n",
    "\n",
    "The definitions of \"large dataset\" and \"high number of features\" are somewhat relative and can vary depending on the context of the problem and the available computational resources. However, as a general rule of thumb, a \"large dataset\" can refer to a dataset with tens of thousands or more samples, while a \"small dataset\" can refer to a dataset with a few hundred or a few thousand samples.\n",
    "\n",
    "Similarly, a \"high number of features\" can also be relative, but it generally refers to datasets where the number of features is comparable to or larger than the number of samples. For example, in genomics and bioinformatics, it is common to work with datasets with tens of thousands or even hundreds of thousands of features.\n",
    "\n",
    "In practice, when working with a large dataset with a high number of features, LinearSVC can be more computationally efficient and can handle such datasets more effectively. On the other hand, when working with a smaller dataset with a lower number of features, SVC(kernel='linear') can perform just as well and may offer better accuracy in some cases.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question XGBoost\n",
    "\n",
    "Random Forest and XGBoost are both popular machine learning algorithms used for regression and classification tasks. However, there are several key differences between these algorithms, which are outlined below:\n",
    "\n",
    "**During the training process**, XGBoost uses gradient boosting to optimize a loss function. The loss function measures the difference between the predicted and actual values and is chosen based on the specific problem being solved. For example, the mean squared error (MSE) loss function can be used for regression problems, and the cross-entropy loss function can be used for classification problems.\n",
    "\n",
    "**Model architecture:** Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to make a final prediction. Each decision tree in the Random Forest is built independently and does not depend on the other trees. On the other hand, XGBoost is a boosted tree algorithm that builds decision trees sequentially, where each new tree corrects the errors of the previous trees.\n",
    "\n",
    "**Tree construction:** In Random Forest, each decision tree is built by randomly selecting a subset of the features and the samples in the training set. This helps to reduce overfitting and improves the performance of the model. In XGBoost, each tree is built by greedily selecting the best split that maximizes the information gain.\n",
    "\n",
    "**Handling of missing data:** Random Forest can handle missing data by imputing the missing values with the mean or median of the feature. XGBoost can handle missing data by splitting the samples into two groups: one group with the missing value and one group without the missing value.\n",
    "\n",
    "**Regularization:** Random Forest does not have any regularization parameters. XGBoost has several regularization parameters, including the learning rate, which controls the step size during the gradient descent, and the regularization term, which penalizes complex models and helps to prevent overfitting.\n",
    "\n",
    "**Performance:** Random Forest is generally faster to train than XGBoost, especially for large datasets. However, XGBoost often outperforms Random Forest in terms of predictive accuracy, especially for complex tasks with high-dimensional features.\n",
    "\n",
    "**In summary,** Random Forest and XGBoost are both powerful machine learning algorithms with different strengths and weaknesses. Random Forest is a simple and fast algorithm that can handle missing data, while XGBoost is a more complex algorithm that can handle complex tasks and has better predictive accuracy, but may require more tuning and training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

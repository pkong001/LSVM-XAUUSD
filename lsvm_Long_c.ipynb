{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize:  True\n",
      "logged in:  True\n",
      "\n",
      "\n",
      "2023-04-17 13:57:23.384102 | Login:  114226077 | Balance:  500.0 | Equity:  500.0\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt5\n",
    "from account_credentials import LOGIN,PASSWORD,SERVER\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "is_initialized = mt5.initialize()\n",
    "print('initialize: ', is_initialized)\n",
    "\n",
    "is_logged_in = mt5.login(LOGIN, PASSWORD, SERVER)\n",
    "print('logged in: ', is_logged_in)\n",
    "print('\\n')\n",
    "account_info = mt5.account_info()\n",
    "print(datetime.now(),\n",
    "    '| Login: ', account_info.login,\n",
    "    '| Balance: ', account_info.balance,\n",
    "    '| Equity: ' , account_info.equity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-01 05:35:00</th>\n",
       "      <td>1760.462</td>\n",
       "      <td>1760.879</td>\n",
       "      <td>1760.442</td>\n",
       "      <td>1760.845</td>\n",
       "      <td>170</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01 05:40:00</th>\n",
       "      <td>1760.833</td>\n",
       "      <td>1761.005</td>\n",
       "      <td>1760.308</td>\n",
       "      <td>1760.493</td>\n",
       "      <td>225</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01 05:45:00</th>\n",
       "      <td>1760.492</td>\n",
       "      <td>1760.527</td>\n",
       "      <td>1759.614</td>\n",
       "      <td>1760.030</td>\n",
       "      <td>270</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01 05:50:00</th>\n",
       "      <td>1760.045</td>\n",
       "      <td>1760.145</td>\n",
       "      <td>1758.333</td>\n",
       "      <td>1758.885</td>\n",
       "      <td>389</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01 05:55:00</th>\n",
       "      <td>1758.869</td>\n",
       "      <td>1759.132</td>\n",
       "      <td>1758.575</td>\n",
       "      <td>1759.064</td>\n",
       "      <td>264</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 06:35:00</th>\n",
       "      <td>2011.347</td>\n",
       "      <td>2012.087</td>\n",
       "      <td>2010.943</td>\n",
       "      <td>2011.265</td>\n",
       "      <td>362</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 06:40:00</th>\n",
       "      <td>2011.240</td>\n",
       "      <td>2012.271</td>\n",
       "      <td>2011.123</td>\n",
       "      <td>2012.246</td>\n",
       "      <td>332</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 06:45:00</th>\n",
       "      <td>2012.249</td>\n",
       "      <td>2012.908</td>\n",
       "      <td>2011.102</td>\n",
       "      <td>2011.473</td>\n",
       "      <td>415</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 06:50:00</th>\n",
       "      <td>2011.441</td>\n",
       "      <td>2011.800</td>\n",
       "      <td>2010.611</td>\n",
       "      <td>2011.743</td>\n",
       "      <td>366</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 06:55:00</th>\n",
       "      <td>2011.715</td>\n",
       "      <td>2012.299</td>\n",
       "      <td>2011.550</td>\n",
       "      <td>2012.217</td>\n",
       "      <td>186</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close  volume  spread  \\\n",
       "date                                                                          \n",
       "2022-08-01 05:35:00  1760.462  1760.879  1760.442  1760.845     170     125   \n",
       "2022-08-01 05:40:00  1760.833  1761.005  1760.308  1760.493     225     125   \n",
       "2022-08-01 05:45:00  1760.492  1760.527  1759.614  1760.030     270     125   \n",
       "2022-08-01 05:50:00  1760.045  1760.145  1758.333  1758.885     389     125   \n",
       "2022-08-01 05:55:00  1758.869  1759.132  1758.575  1759.064     264     125   \n",
       "...                       ...       ...       ...       ...     ...     ...   \n",
       "2023-04-17 06:35:00  2011.347  2012.087  2010.943  2011.265     362     125   \n",
       "2023-04-17 06:40:00  2011.240  2012.271  2011.123  2012.246     332     125   \n",
       "2023-04-17 06:45:00  2012.249  2012.908  2011.102  2011.473     415     125   \n",
       "2023-04-17 06:50:00  2011.441  2011.800  2010.611  2011.743     366     125   \n",
       "2023-04-17 06:55:00  2011.715  2012.299  2011.550  2012.217     186     125   \n",
       "\n",
       "                     real_volume  \n",
       "date                              \n",
       "2022-08-01 05:35:00            0  \n",
       "2022-08-01 05:40:00            0  \n",
       "2022-08-01 05:45:00            0  \n",
       "2022-08-01 05:50:00            0  \n",
       "2022-08-01 05:55:00            0  \n",
       "...                          ...  \n",
       "2023-04-17 06:35:00            0  \n",
       "2023-04-17 06:40:00            0  \n",
       "2023-04-17 06:45:00            0  \n",
       "2023-04-17 06:50:00            0  \n",
       "2023-04-17 06:55:00            0  \n",
       "\n",
       "[50000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = 'XAUUSD'\n",
    "number_of_date= 50000\n",
    "timeframe = mt5.TIMEFRAME_M5\n",
    "from_date = datetime.now()\n",
    "\n",
    "df = pd.DataFrame(mt5.copy_rates_from(symbol,timeframe,from_date,number_of_date))\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "df = df.rename(columns={'time': 'date','tick_volume':'volume'})\n",
    "df = df.set_index(\"date\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding shift windows\n",
    "# targets, check the highest gold price attained in the next 4 hours\n",
    "highs = df['high'].rolling(window=4).max().shift(-4)\n",
    "lows = df['low'].rolling(window=4).min().shift(-4)\n",
    "\n",
    "# create new columns for conditions\n",
    "df['high_close_diff'] = highs - df['close'].shift(1)\n",
    "df['low_close_diff'] = lows - df['close'].shift(1)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49995, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Condition for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2369\n",
      "0 47626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkong\\AppData\\Local\\Temp\\ipykernel_25452\\2025488886.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df.apply(reco,axis=1)\n"
     ]
    }
   ],
   "source": [
    "def reco(row):\n",
    "    if row.low_close_diff <= -3.3:\n",
    "        return 0\n",
    "    elif row.high_close_diff >= 4.3: #I use 3 plus .3 for bit offer off-set, and 1 in case of delay ordering cause price to change.\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['target'] = df.apply(reco,axis=1)\n",
    "\n",
    "\n",
    "print('1', (df['target'] == 1).sum())\n",
    "print('0', (df['target'] == 0).sum())\n",
    "\n",
    "df_simu = df\n",
    "df = df[['open','high','low','close','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49995, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Training, Cross Validation, and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49995\n",
      "number of training set:  34996\n",
      "number of cross validation set:  2500\n",
      "number of test set:  12499\n",
      "sum 49995\n",
      "Training set start: 2022-08-01 05:40:00 \n",
      " Training set end: 2023-01-27 16:00:00\n",
      "cross validation set start: 2023-01-27 16:05:00 \n",
      " cross validation set end: 2023-02-09 17:30:00\n",
      "test set start: 2023-02-09 17:35:00 \n",
      " test set end: 2023-04-17 06:35:00\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(n)\n",
    "\n",
    "\n",
    "train_n = int(round(n*0.70))\n",
    "print('number of training set: ', train_n)\n",
    "\n",
    "cross_n = int(round(n*0.05,0))\n",
    "print('number of cross validation set: ', cross_n)\n",
    "\n",
    "test_n = int(round(n*0.25,0))\n",
    "print('number of test set: ', test_n)\n",
    "\n",
    "print('sum', train_n + cross_n + test_n)\n",
    "\n",
    "\n",
    "train_start = str(df.iloc[0].name)\n",
    "train_end = str(df.iloc[train_n].name)\n",
    "print('Training set start: {0} \\n Training set end: {1}'.format(train_start,train_end))\n",
    "#print(df.loc[train_start:train_end])\n",
    "\n",
    "cross_start = str(df.iloc[train_n+1].name)\n",
    "cross_end = str(df.iloc[train_n+cross_n].name)\n",
    "print('cross validation set start: {0} \\n cross validation set end: {1}'.format(cross_start,cross_end))\n",
    "#print(df.loc[cross_start:cross_end])\n",
    "\n",
    "# test_start = str(df.iloc[train_n + cross_n +1].name)\n",
    "test_start = str(df.iloc[n - test_n+1].name)\n",
    "test_end = str(df.iloc[n-1].name)\n",
    "print('test set start: {0} \\n test set end: {1}'.format(test_start,test_end))\n",
    "#print(df.loc[test_start:])\n",
    "\n",
    "X = df.drop(['target'], axis=1)\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train = np.asarray(X.loc[train_start:train_end])\n",
    "y_train = np.asarray(y.loc[train_start:train_end])\n",
    "\n",
    "X_cross = np.asarray(X.loc[cross_start:cross_end])\n",
    "y_cross = np.asarray(y.loc[cross_start:cross_end])\n",
    "\n",
    "X_test = np.asarray(X.loc[test_start:])\n",
    "y_test =np.asarray(y.loc[test_start:])\n",
    "\n",
    "\n",
    "\n",
    "#Extra for GridSearch\n",
    "X_train_cv = np.asarray(X.loc[train_start:cross_end])\n",
    "y_train_cv = np.asarray(y.loc[train_start:cross_end])\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape, X_cross.shape, y_cross.shape, X_test.shape, y_test.shape, X_train_cv.shape, y_train_cv.shape\n",
    "\n",
    "\n",
    "# Below is for later simulation\n",
    "df_simu_test = df_simu.loc[test_start:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####เอาไว้เรียนรู้ทำความเข้าใจ ตรงนี้สำคัญ ยากและลืมง่าย\n",
    "\n",
    "#Create a sample dataframe\n",
    "df = pd.DataFrame({'value': [2, 4, 5, 7, 6, 8, 9, 10, 11, 12, 9, 8, 6, 4, 3]})\n",
    "\n",
    "#Apply rolling window with current row included and apply max function\n",
    "window_period = 4\n",
    "df['rolling_min'] = df['value'].rolling(window=4).min().shift(-3)\n",
    "df['rolling_max'] = df['value'].rolling(window=4).max().shift(-3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Normalized Data\n",
    "def normalize_data(X):\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm, means, stds\n",
    "\n",
    "#This can run multiple times\n",
    "X_train_n, X_train_means, X_train_stds = normalize_data(X_train)\n",
    "\n",
    "def normalize_data_new(X,means,stds):\n",
    "    X_norm = (X - means) / stds\n",
    "    return X_norm\n",
    "\n",
    "X_cross_n = normalize_data_new(X_cross,X_train_means,X_train_stds)\n",
    "X_test_n = normalize_data_new(X_test,X_train_means,X_train_stds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_long_c.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "sum(X_test_n != X_test_scaled)\n",
    "\n",
    "dump(scaler, 'scaler_long_c.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test import scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12498, 12498, 12498, 12498])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "scaler2 = load('scaler_long_c.pkl')\n",
    "X_test_scaled2 = scaler2.transform(X_test)\n",
    "sum((X_test_n == X_test_scaled2) & (X_test_scaled == X_test_scaled2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean scaler one [1752.03332246 1752.64062957 1751.43216116 1752.03957448]\n",
      "mean scaler two [1752.03332246 1752.64062957 1751.43216116 1752.03957448]\n",
      "mean manual nor [1752.03332246 1752.64062957 1751.43216116 1752.03957448]\n",
      "std scaler one [80.6121871  80.59449647 80.63236456 80.61774939]\n",
      "std scaler two [80.6121871  80.59449647 80.63236456 80.61774939]\n",
      "std manual nor [80.6121871  80.59449647 80.63236456 80.61774939]\n"
     ]
    }
   ],
   "source": [
    "mean = scaler.mean_\n",
    "std = scaler.scale_\n",
    "mean2 = scaler2.mean_\n",
    "std2 = scaler2.scale_\n",
    "print('mean scaler one',mean)\n",
    "print('mean scaler two',mean2)\n",
    "print('mean manual nor',X_train_means)\n",
    "\n",
    "print('std scaler one',std)\n",
    "print('std scaler two',std2)\n",
    "print('std manual nor',X_train_stds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), \"C\" is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "The hyperparameter \"C\" in SVM controls the misclassification penalty. A smaller value of \"C\" allows more misclassifications on the training data, while a larger value of \"C\" penalizes misclassifications more heavily. In other words, a smaller value of \"C\" creates a wider margin, allowing more data points to fall within the margin, but may result in lower accuracy on the training data. Conversely, a larger value of \"C\" creates a narrower margin, reducing the number of misclassifications but may lead to overfitting.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Large C >>> Larger penalizes >>> complex model >>> lead to overfittting <br>\n",
    "Small C >>> Smaller penalizes >>> simple model >>> underfitting\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01, Train score=0.9572249049918564, cross score=0.954\n",
      "C=0.1, Train score=0.957453496013944, cross score=0.954\n",
      "C=0.5, Train score=0.9585678772466212, cross score=0.9556\n",
      "C=1, Train score=0.9593393719461668, cross score=0.9564\n",
      "C=1.5, Train score=0.9595965368460154, cross score=0.9572\n",
      "C=2, Train score=0.9597108323570592, cross score=0.9572\n",
      "C=10, Train score=0.9600251450124296, cross score=0.9584\n",
      "The best 'C' is: 10\n",
      "score: 0.96\n",
      "Accuracy_score:  0.96\n",
      "weight avg 0.94\n",
      "jaccard:  0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      2385\n",
      "           1       0.92      0.10      0.19       115\n",
      "\n",
      "    accuracy                           0.96      2500\n",
      "   macro avg       0.94      0.55      0.58      2500\n",
      "weighted avg       0.96      0.96      0.94      2500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of hyperparameters to loop through\n",
    "#################larger value of C imposes less regularization on the model\n",
    "Cs = [0.01, 0.1,0.5, 1, 1.5, 2, 10]\n",
    "\n",
    "# Initialize variables to keep track of the best hyperparameters and their corresponding score\n",
    "best_score = 0\n",
    "best_C = None\n",
    "max_iter = 20000\n",
    "features_X_train = X_train_n\n",
    "target_y_train = y_train\n",
    "features_X_cross = X_cross_n\n",
    "target_y_cross = y_cross\n",
    "# Loop through the hyperparameters\n",
    "for C in Cs:\n",
    "    lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=C, max_iter=max_iter)\n",
    "    lsvm.fit(X_train_n, y_train)\n",
    "    train_score = lsvm.score(features_X_train, target_y_train)\n",
    "    cross_score = lsvm.score(features_X_cross, target_y_cross)\n",
    "    print(f\"C={C}, Train score={train_score}, cross score={cross_score}\")\n",
    "    \n",
    "    # Update the best hyperparameters and their corresponding score if applicable\n",
    "    if train_score > best_score:\n",
    "        best_score = train_score\n",
    "        best_C = C\n",
    "\n",
    "print(\"The best 'C' is:\", best_C)\n",
    "# Create the LinearSVC model with normalization and the best value of C\n",
    "lsvm = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=best_C, max_iter=max_iter)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lsvm.fit(features_X_train, target_y_train)\n",
    "\n",
    "\n",
    "#EVALUATION\n",
    "test_score = round(lsvm.score(features_X_cross, target_y_cross),2)\n",
    "print(\"score:\", test_score)\n",
    "\n",
    "\n",
    "yhat_m1 = lsvm.predict(features_X_cross)\n",
    "accuracy_m1 = round(accuracy_score(target_y_cross,yhat_m1),2)\n",
    "print('Accuracy_score: ', accuracy_m1)\n",
    "\n",
    "f1 = round(f1_score(target_y_cross, yhat_m1, average='weighted'), 2)\n",
    "print('weight avg', f1)\n",
    "\n",
    "jaccard = round(jaccard_score(target_y_cross, yhat_m1,pos_label=1),2)\n",
    "print('jaccard: ', jaccard)\n",
    "\n",
    "print(metrics.classification_report(target_y_cross, yhat_m1))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(lsvm, features_X_cross, target_y_cross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----cross-----\n",
      "The value should be:>> yhat = 0: 2487<<, >> yhat = 1: 13<<\n",
      "2487\n",
      "13\n",
      "-----test-----\n",
      "12397\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "X_test_simu = X.loc[test_start:]\n",
    "X_test_simu_n = normalize_data_new(X_test_simu,X_train_means,X_train_stds)\n",
    "\n",
    "X_cross_simu = X.loc[cross_start:cross_end]\n",
    "X_cross_simu_n = normalize_data_new(X_cross_simu,X_train_means,X_train_stds)\n",
    "\n",
    "def predict_count(data_simu):\n",
    "    adding = []\n",
    "    for i in range(0,len(data_simu)):\n",
    "        prediction = str(lsvm.predict(data_simu.iloc[i].values.reshape(1,-1))).strip('[]')\n",
    "        adding.append(prediction)\n",
    "    print(adding.count('0'))\n",
    "    print(adding.count('1'))\n",
    "\n",
    "\n",
    "print('-----cross-----')\n",
    "print('The value should be:>> yhat = 0: {0}<<, >> yhat = 1: {1}<<'.format(np.count_nonzero(yhat_m1 == 0),np.count_nonzero(yhat_m1 == 1)))\n",
    "predict_count(X_cross_simu_n)\n",
    "\n",
    "print('-----test-----')\n",
    "predict_count(X_test_simu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simu(model, data_simu_x_n, data_simu_y, initial_capital):\n",
    "    capital = initial_capital\n",
    "    win_count = 0\n",
    "    lost_count = 0\n",
    "    no_order_count = 0\n",
    "    record_result = []\n",
    "    record_capital = []\n",
    "    record_date = []\n",
    "    consecutive_lost = 0\n",
    "    consecutive_lost_max = 0\n",
    "    for i in range(0, len(data_simu_x_n)):\n",
    "        prediction = model.predict(data_simu_x_n.iloc[i].values.reshape(1,-1))\n",
    "        \n",
    "        if prediction == 1 and data_simu_y[i] == 1:\n",
    "            outcome = 3\n",
    "            capital += outcome\n",
    "            win_count += 1\n",
    "            order_record = 'win-------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'win'\n",
    "        elif prediction == 1 and data_simu_y[i] == 0:\n",
    "            outcome = -3.3\n",
    "            capital += outcome\n",
    "            lost_count += 1\n",
    "            order_record = 'lost------- prediction = {0}, actual = {1}'.format(prediction,data_simu_y[i])\n",
    "            result = 'lost'\n",
    "        elif prediction == 0:\n",
    "            no_order_count +=1\n",
    "            order_record = 'no order--- prediction = {0}, actual = {1}'.format(prediction, data_simu_y[i])\n",
    "            capital = capital\n",
    "            result = 'no order'\n",
    "        else:\n",
    "            raise ValueError('no condition met')\n",
    "        record_date.append(data_simu_x_n.iloc[i].name)\n",
    "        record_result.append(order_record)\n",
    "        record_capital.append(capital)\n",
    "\n",
    "        #Calculate Consecutive Lost\n",
    "        if result == 'lost':\n",
    "            consecutive_lost += -3.3\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "        if result == 'win':\n",
    "            if consecutive_lost <= consecutive_lost_max:\n",
    "                consecutive_lost_max = consecutive_lost\n",
    "            consecutive_lost = 0\n",
    "\n",
    "    total_return = ((capital - initial_capital) / initial_capital)*100\n",
    "    sim_df = pd.DataFrame({'record_date':record_date,\n",
    "                           'record_result':record_result,\n",
    "                           'record_capital': record_capital})\n",
    "    sim_df.set_index('record_date', inplace=True)\n",
    "\n",
    "    return win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Return: 44.0%\n",
      "Final Capital: $721.0\n",
      "Win Count:  88\n",
      "Lost Count:  13\n",
      "No Order Count:  12397\n",
      "Max Consecutive Lost: 1.0 trades\n",
      "Total day on trading: 67 days\n",
      "accuracy 0.87%\n"
     ]
    }
   ],
   "source": [
    "win_count, lost_count, no_order_count, capital, total_return, sim_df, consecutive_lost_max = predict_simu(lsvm, \n",
    "                                                                                                          X_test_simu_n, \n",
    "                                                                                                          y_test, \n",
    "                                                                                                          500)\n",
    "print('Total Return: {0}%'.format(round(total_return,0)))\n",
    "print('Final Capital: ${0}'.format(round(capital,0)))\n",
    "print('Win Count: ',win_count)\n",
    "print('Lost Count: ',lost_count)\n",
    "print('No Order Count: ', no_order_count)\n",
    "print('Max Consecutive Lost: {0} trades'.format(consecutive_lost_max/-3.3))\n",
    "day_one = X_test_simu_n.iloc[0].name.to_pydatetime().date()\n",
    "day_final = X_test_simu_n.iloc[len(X_test_simu_n)-1].name.to_pydatetime().date()\n",
    "print('Total day on trading: {0} days'.format((day_final - day_one).days))\n",
    "accuracy = win_count/(win_count+lost_count)\n",
    "print(f\"accuracy {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.plot(y='record_capital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the simulation data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_long_simu_c.csv', y_test, delimiter=',')\n",
    "X_test_simu_n.to_csv('X_long_simu_c.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To import the file use:\n",
    "\n",
    "```python\n",
    "y_test = np.loadtxt('y_long_simu_c.csv', delimiter=',')\n",
    "X_test_simu_n = pd.read_csv('X_long_simu_c.csv',index_col='datetime', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version should be 4.0: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('version should be 4.0:',pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsvm,open('lsvm_xauusd_long_c.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('lsvm_xauusd_long_c.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = [-2.839808,-2.806494,-2.828501,-2.791640]\n",
    "model_input = np.array(model_input).reshape(1,-1)\n",
    "model_input\n",
    "pickled_model.predict(model_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question on SVM\n",
    "\n",
    "##### what is the different between svm.SVC(kernel='linear') and LinearSVC in sklearn?\n",
    "\n",
    "svm.SVC(kernel='linear') and LinearSVC are both linear SVM classifiers available in scikit-learn, but there are some differences between them.\n",
    "\n",
    "svm.SVC(kernel='linear') is based on the LIBSVM library, which is written in C++, while LinearSVC is based on the LIBLINEAR library, which is written in C.\n",
    "\n",
    "One of the main differences between these two is the handling of multi-class classification. svm.SVC(kernel='linear') uses the one-vs-one approach, which trains a binary classifier for every pair of classes, while LinearSVC uses the one-vs-the-rest approach, which trains a binary classifier for each class against all the other classes.\n",
    "\n",
    "Another difference is the way they handle the regularization parameter C. In svm.SVC(kernel='linear'), C controls the tradeoff between maximizing the margin and minimizing the classification error. In LinearSVC, C controls the strength of regularization, where smaller values of C lead to stronger regularization.\n",
    "\n",
    "Overall, LinearSVC is faster and more memory-efficient than svm.SVC(kernel='linear') and is recommended for large datasets with many features, while svm.SVC(kernel='linear') may be more suitable for smaller datasets with few features or for multi-class classification.\n",
    "\n",
    "##### which is created first\n",
    "\n",
    "SVC(kernel='linear') was created first and it is a more flexible implementation of linear support vector machines. It can handle non-linearly separable data by using the kernel trick to transform the input data into a higher-dimensional feature space where the data becomes linearly separable.\n",
    "\n",
    "LinearSVC, on the other hand, is a more efficient implementation of linear support vector machines that only supports linear kernels. It uses a different algorithm for training the model that is more scalable and faster than the algorithm used by SVC(kernel='linear').\n",
    "\n",
    "Both models are useful for solving binary classification problems, but LinearSVC is generally preferred when the dataset is large and the number of features is high, while SVC(kernel='linear') is preferred when the dataset is smaller or when non-linearly separable data is involved.\n",
    "\n",
    "##### Definition of large dataset and high number of features\n",
    "\n",
    "The definitions of \"large dataset\" and \"high number of features\" are somewhat relative and can vary depending on the context of the problem and the available computational resources. However, as a general rule of thumb, a \"large dataset\" can refer to a dataset with tens of thousands or more samples, while a \"small dataset\" can refer to a dataset with a few hundred or a few thousand samples.\n",
    "\n",
    "Similarly, a \"high number of features\" can also be relative, but it generally refers to datasets where the number of features is comparable to or larger than the number of samples. For example, in genomics and bioinformatics, it is common to work with datasets with tens of thousands or even hundreds of thousands of features.\n",
    "\n",
    "In practice, when working with a large dataset with a high number of features, LinearSVC can be more computationally efficient and can handle such datasets more effectively. On the other hand, when working with a smaller dataset with a lower number of features, SVC(kernel='linear') can perform just as well and may offer better accuracy in some cases.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question XGBoost\n",
    "\n",
    "Random Forest and XGBoost are both popular machine learning algorithms used for regression and classification tasks. However, there are several key differences between these algorithms, which are outlined below:\n",
    "\n",
    "**During the training process**, XGBoost uses gradient boosting to optimize a loss function. The loss function measures the difference between the predicted and actual values and is chosen based on the specific problem being solved. For example, the mean squared error (MSE) loss function can be used for regression problems, and the cross-entropy loss function can be used for classification problems.\n",
    "\n",
    "**Model architecture:** Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to make a final prediction. Each decision tree in the Random Forest is built independently and does not depend on the other trees. On the other hand, XGBoost is a boosted tree algorithm that builds decision trees sequentially, where each new tree corrects the errors of the previous trees.\n",
    "\n",
    "**Tree construction:** In Random Forest, each decision tree is built by randomly selecting a subset of the features and the samples in the training set. This helps to reduce overfitting and improves the performance of the model. In XGBoost, each tree is built by greedily selecting the best split that maximizes the information gain.\n",
    "\n",
    "**Handling of missing data:** Random Forest can handle missing data by imputing the missing values with the mean or median of the feature. XGBoost can handle missing data by splitting the samples into two groups: one group with the missing value and one group without the missing value.\n",
    "\n",
    "**Regularization:** Random Forest does not have any regularization parameters. XGBoost has several regularization parameters, including the learning rate, which controls the step size during the gradient descent, and the regularization term, which penalizes complex models and helps to prevent overfitting.\n",
    "\n",
    "**Performance:** Random Forest is generally faster to train than XGBoost, especially for large datasets. However, XGBoost often outperforms Random Forest in terms of predictive accuracy, especially for complex tasks with high-dimensional features.\n",
    "\n",
    "**In summary,** Random Forest and XGBoost are both powerful machine learning algorithms with different strengths and weaknesses. Random Forest is a simple and fast algorithm that can handle missing data, while XGBoost is a more complex algorithm that can handle complex tasks and has better predictive accuracy, but may require more tuning and training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
